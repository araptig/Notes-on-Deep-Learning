\documentclass[onecolumn]{IEEEtran}
%\documentclass{standalone}

\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bm}
\usepackage{longtable}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{youngtab}
\usepackage{hyperref}
\usepackage{bookmark}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{verbatim}
\usepackage{etoolbox}

\tikzstyle{block} = [draw, fill=blue!20, rectangle,
    minimum height=3em, minimum width=6em]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\setcounter{page}{1}
\footnoterule \footnotesep 10mm
\overfullrule 5pt
\vbadness=5000
\hbadness=5000

%\makeatletter
%\preto{\@verbatim}{\topsep=0pt \partopsep=0pt}
%\def\verbatim@font{\linespread{1}\normalfont\ttfamily}
%\makeatother

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\begin{document}
\title{Notes on Machine Learning \& Deep Learning}
\author{Ara Patapoutian, 2017}
\maketitle


\def\ruleone{\rule[-1.5ex]{0cm}{4.0ex}}
\vspace{1cm}

\tableofcontents
\newpage

\section{Introduction\label{intro_section}}
\begin{itemize}
    \item Computer science, artificial intelligence, machine learning \& neural networks:
    \bi
        \item \emph{computer science}, or CS, is the study of the principles and use of computers
        \item \emph{artificial intelligence}, or AI, is a subfield of CS that studies tasks that normally require human intelligence
        \item \emph{machine learning}, or ML, is a subfield of AI that evolved from the study of pattern recognition and computational learning theory
        \item \emph{neural networks}, or NN, ia a subfield ML that is inspired by biological neural networks
        \beq\begin{CD}
                \text{CS}  @>>>  \text{AI} @>>> \text{ML}  @>>> \text{NN}
        \end{CD}\eeq
    \ei
    \item Machine learning:
    \bi
        \item ML is the science of getting computers to act without being explicitly programmed
        \item ML is applied on problems where
        \begin{enumerate}
            \item data is available
            \item a pattern or correlations exist
            \item mathematical formulation is not practical or possible
        \end{enumerate}
    \ei
    \item Learning:
    \bi
        \item in nature, cause precedes effect
        \beq
            \text{cause}  \rightarrow   \text{effect}
        \eeq
        in learning, observations lead to constructing cause
        \beq
            \text{effect} \rightarrow \text{cause}
        \eeq
        \item formalization of learning $(\mathcal{H},\texttt{A})$:
        \bi
            \item $\mathcal{H}=\{h_i\}$ is a hypothesis set,
            \item a learning algorithm, \texttt{A}, picks the \emph{final hypothesis} $g$
            \beq\label{hypotheses}\begin{CD}
                \mathcal{H}=\{h_i\}  @>A>>  g \in \mathcal{H}
            \end{CD}\eeq
        \ei
        \item learning versus memorization:
        \bi
            \item learning allows for generalization, whereas memorization does not, see Section \ref{gen_section}
        \ei
    \ei
    \item Paradigms of learning:
    \begin{enumerate}
        \item supervised learning, Sections \ref{fnn_section} and \ref{rnn_section}
        \bi
            \item supervised learning implies data is available for training with the desired outcome
            \item in classification, the correct outcome is referred to as the \emph{label}
            \item at present, supervised learning is the prevalent learning paradigm
        \ei
        \item unsupervised learning, Section \ref{unsupervised_section}
        \bi
            \item no labels available during training
        \ei
        \item reinforcement learning,
        \bi
            \item no supervision, only a reward system
            \item feedback is delayed, not instantaneous
            \item for applications where time matters, i.e. sequential, not i.i.d.
            \item e.g. backgammon
        \ei
    \end{enumerate}
    \item Neural networks:
    \bi
        \item a neural network is sometimes referred to as  \emph{artificial neural network} or ANN
        \item NN is hands-on, empirical field for now
        \bi
            \item experimentation over deep thoughts
        \ei
        \item a NN is composed of neurons, see Section \ref{neuron_section}
        \item little is known about biological learning to provide guidance for learning of NNs
        \item NNs come in two flavors:
        \bi
            \item feedforward, see Section \ref{fnn_section}, and
            \item recurrent, see  Section \ref{rnn_section}
        \ei
        \item NNs have been successful in recent years and are the main focus of this write-up \cite{lecun2015}
    \ei
    \item Factors of variation:
    \bi
        \item when analyzing a speech, the \emph{factors of variation} include the speaker’s age, their sex, their accent and the words that they are speaking
        \item when analyzing an image of a car, the factors of variation include the position of the car, its color, and the angle and brightness of the sun
        \item unfortunately, many of the factors of variation influence every single piece of data we are able to observe
        \item it is usually needed to disentangle the factors of variation and discard the ones that we do not care about
    \ei
    \item Features in a representation:
    \bi
        \item the performance of a machine learning algorithm depends on the \emph{representation} of the data they are given
        \item each piece of information included in the representation is known as a \emph{feature}
        \item feature extraction from input data is nonlinear
        \item features reduce the input data dimensionality by trying to extract useful information
    \ei
    \item Template matching:
    \bi
        \item template matching is a simple feature extractor, that may work if factors of variations are limited
    \ei
    \item Representation learning:
    \bi
        \item externally identifying relevant features is difficult
        \item current philosophy is to use NN to discover not only the mapping from representation to output but also the representation itself
        \item this approach is known as \emph{representation learning}
    \ei
\end{itemize}


\subsection{Probabilistic formulation\label{prob_sec}}
\begin{itemize}
    \item Interpreting NN as generating a conditional probability: target distribution
    \bi
        \item consider a NN with supervised learning, where the data \& label pairs $\{(\bm{x}_n,\bm{y}_n)\}$, are available
        \bi
             \item $\bm{x}_n$ is a $d$ dimensional random vector, representing the input
             \item $\bm{y}_n$ is a $d_o$ dimensional vector, representing the observed output,
        \ei
        \item a NN models the conditional probability of the map
        \beq\label{input_output}\begin{CD}
            \bm{x}_n  @>P(\bm{y}|\bm{x})>> \bm{y}_n
        \end{CD}\eeq
        where $P(\bm{y}|\bm{x})$ is a conditional probability function
        \item $P(\bm{x})$ \& $P(\bm{y}|\bm{x})$ are in general unknown
        \item $(\bm{x}_n,\bm{y}_n)$ can be generated from $P(\bm{x},\bm{y})$, where
        \beq
            P(\bm{x},\bm{y}) = P(\bm{x})\, P(\bm{y}|\bm{x})
        \eeq
        \item $P(\bm{y}|\bm{x})$, is referred to as the \emph{target distribution}
        \bi
            \item the target distribution is the quantity that needs to be learned
        \ei
    \ei
    \item Equivalent formulation: target function
    \bi
        \item the target distribution can be interpreted as a deterministic mapping, known as the \emph{target function}
        \beq\label{conditional_mean}
            f(\bm{x}_n) \triangleq E_{y}(\bm{y}|\bm{x}_n)
        \eeq
        together with some \emph{stochastic noise}
        \beq\label{noise_def}
            \bm{n}_n \triangleq \bm{y}_n - f(\bm{x}_n)
        \eeq
        \item stochastic noise is a characteristic of the environment and is independent of the NN
        \item under this formulation,  the goal of a NN is to learn $f(\bm{x})$ (\ref{conditional_mean}), from $\{(\bm{x}_n,\bm{y}_n)\}$
        \item target function is assumed to be unknown
    \ei
    \item Classification versus regression:
    \bi
        \item in a \emph{classification} problem, $\{y_n\}$ is a finite set
        \item in a \emph{regression} problem, $\{y_n\}$ is a subset of the real number
        \bi
            \item regression means real-valued output
        \ei
    \ei
 \end{itemize}


\subsection{Datasets\label{data_set_section}}
\begin{itemize}
    \item Obtaining large repository of data, along with talent, are the two scarce resources
    \bi
        \item this is a major bottleneck for startups
        \item some products are launched with the purpose of collecting data rather than revenue
    \ei
    \item Mean \& variance:
    \bi
        \item consider sample data set $\{\bm{x}_n\}$, of \emph{sample size} $N$
        \item arrange the data set $\{\bm{x}_n\}$ into an $(N \times d)$ \emph{data matrix} $X$, whose $\text{n}^\text{th}$ rows is $\bm{x}_n$
        \item the mean $\bar{\bm{x}}$, associated with $\{\bm{x}_n\}$, is
        \beq\label{data_mean}\begin{split}
            \bar{\bm{x}} &= \frac{1}{N} \sum_{n \leq N}\bm{x}_n\\
            &= \frac{1}{N} \bm{1}^T X
        \end{split}\eeq
        where $\bm{1}$ is $(N \times 1)$ vector of $1$s
        \item the covariance $\bar{C}$, associated with $X$ is
        \beq\label{data_cov}\begin{split}
            \bar{C} &= \frac{1}{N} \sum_{n \leq N} (\bm{x}_n - \bar{\bm{x}}) (\bm{x}_n - \bar{\bm{x}})^T\\
            &= \frac{X^TX}{N} - \bar{\bm{x}} \bar{\bm{x}}^T
        \end{split}\eeq
    \ei
    \item Dataset modifications:
    \bi
        \item various modifications can be applied to a dataset, including
        \bi
            \item preprocessing
            \item normalization
            \item expansion, or augmentation
            \item reuse
        \ei
    \ei
    \item Data preprocessing:
    \bi
        \item dataset $\{\bm{x}_n\}$ is usually modified before applying to a NN, by making it zero mean,
        \beq\label{data_manipulation}
            \hat{\bm{x}}_n \triangleq  \bm{x}_n - \bar{\bm{x}}
        \eeq
        \item it is possible to reduce data dimensionality ($d$), using \emph{principle component analysis}, or PCA (\ref{pca_b})
        \item a  dataset $\{\bm{x}\}$ can be \emph{whitened} by normalizing the variance on each component by dividing each component by the square root of its eigenvalue
        \item for images, usually only mean centering is performed
    \ei
    \item Batch normalization:
    \bi
        \item in practice the data-set is partitioned to \emph{mini-batches} and the NN is updated one mini-batch at a time
        \bi
            \item denote the mini-batch size by $m$
        \ei
        \item with \emph{batch normalization}, every layer $k$, of a network is augmented with another normalizing layer
        \item each layer performs the following normalization:
        \bi
            \item determine the batch mean $\bm{\mu}_k$ (\ref{data_mean}) \& variance $\bm{\sigma}^2_k$
            \item given scale \& shift parameters $(\gamma^{(k)}, \beta^{(k)})$ compute
            \beq\label{bn}\begin{CD}
                \bm{x}_k @>>> \hat{\bm{x}}_k \triangleq \frac{\bm{x}_k- \bm{\mu}_k}{\bm{\sigma}_k} @>>>  \bm{y}_k \triangleq \gamma^{(k)}  \hat{\bm{x}}_k + \beta^{(k)}
            \end{CD}\eeq
        \ei
    \ei
    \item Data expansion or augmentation:
    \bi
        \item the training data  $\{(\bm{x}_n,\bm{y}_n)\}$, can be artificially expanded by applying, on the available data, application-dependent transformations
        \bi
            \item for image recognition, can use
            \bi
                \item translation
                \item rotation
                \item horizontal flip
                \item random crops \& scales and then rescale
                \item deformation, distortion, stretch and sheer
                \item color jitter such as contrast, brightness, saturation
            \ei
            \item for speech recognition, can introduce background interference, change the speed etc.
        \ei
    \ei
    \item Data snooping:
    \bi
        \item if a data set  has affected any step in the learning process, its ability to assess the outcome has been compromised
        \item this phenomenon is known as \emph{data snooping}
        \item data reuse and expansion should be carried out carefully
        \item data snooping is a common trap for a practitioner
    \ei
    \item Data partitioning:
    \bi
        \item data is partitioned into training, testing and validation, see Section \ref{gen_section}
    \ei
\end{itemize}


\section{Fundamentals}
\subsection{Cost functions}
\begin{itemize}
    \item General remarks:
    \bi
        \item learning is accomplished by minimizing some cost function
        \bi
            \item or equivalently by reward maximization
        \ei
        \item cost functions are also referred to as \emph{error metrics}, \emph{lost functions}, or \emph{objective functions}
    \ei
    \item Pointwise cost function:
    \bi
        \item for a given input $\bm{x}$, the pointwise \emph{cost function} between hypotheses $h$ \& $f$ is
        \beq
            c_{h,y}\triangleq c\left(\bm{y}^{(h)},\bm{y}^{(f)}\right)
        \eeq
        where
        \bi
            \item $\bm{y}^{(h)} \triangleq h(\bm{x})$ is the output associated with hypothesis $h$
            \item such a cost function can be applied for all learning paradigms
            \item when the label $\bm{y}$ associated with data $\bm{x}$ is available, the cost function associated with hypothesis $h$ becomes
            \beq\label{p_cf}
                c_h \triangleq c\left(\bm{y}^{(h)},\bm{y}\right)
            \eeq
        \ei
    \ei
    \item Cost function examples:
    \begin{enumerate}
        \item The \emph{Hamming distance}, defined as
        \beq
            c_{h,f} \triangleq [h(\bm{x}) \neq f(\bm{x})]
        \eeq
        is often used when $f(\bm{x}) \in \{0, 1\}$
        \item The \emph{generalized Euclidean distance} is defined as
        \beq\label{gen_metric}
            c_{h,f} \triangleq (\bm{y}^{(h)} - \bm{y}^{(f)})^T Q \, (\bm{y}^{(h)} - \bm{y}^{(f)})
        \eeq
        where $Q$ is an arbitrary positive semi-definite matrix  \newline
        two special classes include
        \bi
            \item the \emph{Euclidean distance}, when $Q=I$
            \item the \emph{Mahalanobis distance}, when $Q$ is the inverse of the covariance matrix of the data
            \bi
                \item the Mahalanobis distance metric depends on the data set
                \item the main advantage of the Mahalanobis distance over the standard Euclidean distance is that it takes into account correlations among the data dimensions and scale
            \ei
        \ei
        \item The log-likelihood cost function:
        \bi
            \item the log-likelihood cost function, from the ML criterion (\ref{ml_criterion}), is given by
            \beq
                \log_2 \frac{1}{P(y|\bm{x})}
            \eeq
            \item since a NN can model $P(y|\bm{x})$  (\ref{input_output}), using hypothesis $h$ (\ref{hypotheses}, \ref{likelihood_notation}), the log-likelihood cost function is expressed as
            \beq\begin{split}\label{llcf}
                c_h &= \log_2 \frac{1}{h(\bm{x})}\\
                &= - \log_2 h(\bm{x})
            \end{split}\eeq
            \item the log-likelihood cost function is often used with softmax activation (\ref{softmax_def})
        \ei
        \item The \emph{cross-entropy} cost function (\ref{cross_entropy_der}):
        \bi
            \item the cross-entropy measure is a useful metric when the NN is estimating the probability distribution associated with a classification problem
            \item then the label $\bm{y}$ represents a probability distribution
            \bi
                \item $\bm{y}=p(\bm{x})$ is the desired probability distribution
                \item $\hat{\bm{y}}= h(\bm{x})$ is the estimated probability distribution
            \ei
            \item  from (\ref{cross_entropy_der}), the cross-entropy cost function associated with data $(\bm{x},\bm{y})$ and hypothesis $h$ is
             \beq\label{cross_entropy_def_rep}
                c_h =  -\sum_i y_i \log_2 h_i(\bm{x})
            \eeq
            \item with \emph{disjoint classification} problems only a single outcome is valid, \& $\bm{y}$ becomes a one-hot vector
            \bi
                \item the cross-entropy simplifies to
                \beq
                    c_h \triangleq - \log_2 h_i(\bm{x})
                \eeq
                \item the above cost coincides with the log-likelihood cost function (\ref{llcf})
            \ei
        \ei
        \item Multi-class SVM loss function:
        \bi
            \item consider a margin of $\vartriangle$, as described in Section \ref{svm_sec}
            \item given $(\bm{x}_i, y_i)$, define the score of the $j^\text{th}$ class as
            \beq
                s_j \triangleq  f_j(\bm{x}_i, \bm{w}),
            \eeq
            \item the \emph{multi-class SVM loss}, or \emph{hinge loss},  for data $(\bm{x}_i, y_i)$ is defined as,
            \beq
                c_i \triangleq \sum_{j \neq y_i} \max(0,s_j-s_{y_i}+\vartriangle)
            \eeq
        \ei
        \item The \emph{cosine similarity metric}, $\in [−1, 1]$, is defined as
        \beq
            c_{h,f} \triangleq \frac{\bm{y}^{(h)}\cdot \bm{y}^{(f)}}{\|\bm{y}^{(h)}\|\|\bm{y}^{(f)}\|}
        \eeq
        \item The \emph{Jaccard coefficient} is often used when the objects represent sets,
        \beq
                c(S1, S2) \triangleq \frac{|S1 \cap S2|}{|S1 \cup S2|}
        \eeq
    \end{enumerate}
    \item In-sample versus out-of-sample cost functions:
    \bi
        \item using (\ref{p_cf}), the \emph{in-sample cost function} $C_\text{in}(h)$, over $N$ samples, of using hypothesis $h$ is defined as
        \beq\label{in_sample}
            C_\text{in}(h)  \triangleq \frac{1}{N}\sum_{n=1}^N c(\bm{y}_n^{(h)},\bm{y}_n)
        \eeq
        \item the \emph{out-of-sample} cost function $C_\text{out}(h)$  is defined as
         \beq\label{out_sample}
            C_\text{out}(h) \triangleq E_{x,y} [c(\, h(X),\,Y)]
        \eeq
    \ei
\end{itemize}


\subsection{Neuron models\label{neuron_section}}
\begin{itemize}
    \item A neuron:
    \bi
        \item a neuron is also referred to as a \emph{unit} or a \emph{node}
        \item a neuron takes a $(d+1)$ dimensional input vector $\bm{x}$, and outputs a scalar $y$
        \item a neuron is some linear circuit followed by a non-linear function,
        \beq\label{linear_sum}\begin{CD}
            \bm{x} @>\bm{w}>> s\triangleq \bm{w}^T \bm{x} @>\theta>> y \triangleq \theta(s)
        \end{CD}\eeq
        \bi
            \item the linear portion is modeled with the parameter vector $\bm{w}$, called the \emph{weight}
            \bi
                \item \emph{linearity in the weights} means the weights have linear dependency
                \item $s$ is a scalar called the \emph{signal} or the \emph{score}
            \ei
            \item the non-linearity is introduced through the \emph{activation function} $\theta$
        \ei
        \item a neuron can also be defined in a more setting
        \beq\label{activation_fn}
            y \triangleq \theta(\bm{x}, \bm{w})
        \eeq
        \item most neurons considered here have a $(d+1)$ dimensional $\bm{w}$, with $x^0 \triangleq 1 \Rightarrow w^0$ is the \emph{bias}
        \item a hypothesis $h$ is a function of weights $\bm{w}$
    \ei
    \item Non-linearities \& Activation functions:
    \bi
        \item deeper networks can generate more sophisticated non-linear transformations
        \item various activation functions can be applied on $s$ (\ref{linear_sum})
        \item in general, activations need to be monotonically non-decreasing, and mostly differentiable
    \ei
    \item Feature transform:
    \bi
       \item  in addition to activation functions, non-linearities could also be introduced by non-linear mapping of the input data
        \item a \emph{feature transform} $\Phi$, is a non-linear transformation
        \beq\begin{CD}
            \bm{x}= (x_0, \cdots , x_d) @>\phi>> \bm{z}= (z_0, \cdots , z_{\tilde{d}})
        \end{CD}\eeq
        where $\bm{z} \in \mathbb{R}^{\tilde{d}+1}$ is in the \emph{feature space}
        \item strictly speaking $\Phi$ may not satisfy the constraints of a function
        \item the linear summation (\ref{linear_sum}) generalizes to
        \beq\label{z_space}
              \bm{\tilde{w}}^T \bm{z} = \bm{\tilde{w}}^T \Phi(\bm{x})
        \eeq
        \item linearity in weights is retained since non-linearities are applied on the inputs, not the weights,
        \item if $\tilde{d} > d$, then we might have generalization concerns
        \item if $\tilde{d} < d$, then we are acting as a learning machine, and should account for that for generalization
        \item latest approaches rely on deeper NN rather than feature transformations to obtain sophisticated non-linearities
    \ei
    \item Identity, linear classifiers \& cybernetics:
    \bi
        \item in certain networks, a fraction of the neurons do not have activation functions
        \item in this case, $\theta$ collapses to the identity function, and the \emph{linear regression} hypotheses is obtained, see Section \ref{math_section}
            \beq
                \theta(s) = s = \bm{w}^T \bm{x}
            \eeq
        \item if a single neuron is used, a hypothesis can then be determined by the $(d+1)$ parameters $\bm{w}=\{w_i\}$, and the signal $s$ (\ref{linear_sum}) itself is used to classify inputs
        \bi
            \item \emph{cybernetics} implied learning $\bm{w}$ of such a model
        \ei
    \ei
    \item List of activation functions:
    \begin{enumerate}
        \item Perceptron:
        \bi
            \item the \emph{McCulloch-Pitts Neuron}, $1943$,  is a binary classifier, obtained by setting the activation function to be the sign() of the input value
            \beq\label{perceptron}
                \theta_{MP}(s) \triangleq \text{sign}(s) = \text{sign}(\bm{w}^T \bm{x})
            \eeq
            \item  a \emph{perceptron} \cite{ros1958} is a McCulloch-Pitts Neuron that can learn its weights
            \item since the derivative of this activation vanishes, it is not a useful activation for learning
            \item based on Section \ref{vc_section} a complexity measure can be associated with the perceptron
            \bi
                \item theorem: $d_{VC} = d+1$, which coincides with the number of parameters $w_i$
            \ei
        \ei
        \item The \emph{softsign} function is the soft version of the McCulloch-Pitts Neuron:
        \beq
              \theta_{SS}(s) \triangleq \frac{s}{1+|s|},
        \eeq
        \item The logistic sigmoid function:
        \bi
            \item the \emph{logistic sigmoid} function:
            \beq\label{sigmoid}
                \theta_L(s) \triangleq \frac{e^s}{1+e^s} = \frac{1}{1+e^{-\bm{w}^T \bm{x}}}
            \eeq
            \item the derivative is
            \beq
                 \dot{\theta}_L(s) =  \theta_L(s) \, (1- \theta_L(s))
            \eeq
            \item $\theta_L(s) \geq 0 \Rightarrow x_i \geq 0$, (i.e. not centered around zero) implies the changes of $w_i$, due to gradient, are all positive or negative, see sub-section \ref{stochastic_sec}
            \item furthermore, saturated regions have no gradient information
            \item useful for gating functions
        \ei
        \item Stochastic neuron \& logistic regression:
        \bi
           \item a boolean variable $y \in \{0,1\}$ can be referred to as an \emph{indicator variable}
           \item in \emph{logistic regression}, the goal is to determine the conditional probability of an indicator variable, $P(y=1|\bm{x})$
           \item since the output of (\ref{sigmoid}) is $\in [0,1]$, this output can be reinterpreted as a conditional probability, $P(y=1|\bm{x})$
           \item a \emph{stochastic neuron} has the same activation function as the the logistic sigmoid, $\theta_L(s)$ that implements a logistic regression
           \bi
                \item the goal is to estimate $P(y=1|\bm{x})$ rather than $y$
           \ei
           \item this is similar to obtaining soft data in data communication
           \item in general $P(y=1|\bm{x})$ is not a linear function with respect to $\bm{x}$
            \item logistic regression converts the problem to a linear regression (with infinite range), by \emph{assuming} the transformation
            \beq\begin{split}
                \log \frac{P(y=1|\bm{x})}{1-P(y=1|\bm{x})} &= \bm{w}^T \bm{x} = s\Rightarrow\\
                P(y=1|\bm{x}) &= \frac{e^s}{1+e^s},
            \end{split}\eeq
            \item note that this coincides with (\ref{sigmoid}), and that
            \beq\begin{split}
                 \theta(-s) &=  \frac{1}{1+e^s}\\
                &= 1 - \frac{e^s}{1+e^s}\\
                &= 1 - \theta(s)\\
                &= P(y=0|\bm{x})
            \end{split}\eeq
            \item both $P(y=1|\bm{x})$ \& $P(y=0|\bm{x})$ can be expressed by the unified form
            \beq\label{likelihood_notation}
                P(y|\bm{x}) = \theta(y\bm{w}^T \bm{x})
            \eeq
        \ei
        \item The \emph{tangent hyperbolic} function:
        \beq\label{tanh_fn}
                \theta_T(s) \triangleq \tanh(s)  = \frac{e^s-e^{-s}}{e^s+e^{-s}}
        \eeq
        \bi
            \item popular in $1980$s
            \item the range is $\theta_T \in [-1,1]$
            \item $\theta_T(s)$ can be obtained from $\theta_L$ (\ref{sigmoid}) by the transformation $y = 2x-1$
        \ei
        \item The \emph{hard tanh} function:
        \beq
            \theta_{HT}(s) \triangleq \begin{cases}
                          -1, & \mbox{if } s < -1 \\
                          s, & \text{if } -1 \leq s \leq 1 \\
                          1, & \mbox{if } s > 1
                        \end{cases}
        \eeq
        \item The \emph{rectified linear unit}, or ReLU:
            \beq\label{relu}\begin{split}
                 \theta_R(s) &\triangleq \max(0,s) \\
                 &= \max(0, \bm{w}^T \bm{x})
            \end{split}\eeq
        \bi
            \item currently a popular activation function
            \item typically learns faster in networks with many layers
            \item when the signal stays negative, we have a \emph{dead ReLU}
            \bi
                \item positive bias initialization addresses the dead ReLU concern
            \ei
        \ei
        \item The \emph{parametric ReLU}, or PReLU:
        \beq
            \theta_{PR}(s) \triangleq \max(\alpha s,s)
        \eeq
        \bi
            \item setting $\alpha = 0.01$ gives the \emph{leaky ReLU}:
            \beq
                \theta_{LR}(s) \triangleq \max(0.01 s,s)
            \eeq
        \ei
        \item The \emph{exponential linear unit}, or \emph{ELU}:
        \beq
            \theta_{ELU}(s) \triangleq \begin{cases}
                          s, & \mbox{if } s>0 \\
                          \alpha (e^s -1), & \mbox{if } s \leq 0
                        \end{cases}
        \eeq
        \item The \emph{softplus} function is smooth (soft) version of (\ref{relu}):
        \beq
            \theta_{SP}(s) \triangleq \log (1 + \exp(x))
        \eeq

        \item The \emph{Maxout} activation:
        \beq
            \theta_{MO}(\bm{x}) \triangleq \max (\bm{w}_1^T \bm{x},\bm{w}_2^T \bm{x})
        \eeq
        \item The \emph{softmax} function, with $1 \leq j \leq K$ output nodes, is
        \beq\label{softmax_def}
            \theta_{SM}(j; s_1^K) \triangleq \frac{e^{s_j}}{\sum_{k=1}^{k=K} e^{s_k}}
        \eeq
        where the $j^\text{th}$ node generates the probability of the $j^\text{th}$ event
        \bi
            \item since $\theta(j) \geq 0$ and $\sum_j \theta(j)=1$, we can also interpret $\{ \theta(j)\}$ as probabilities
            \item softmax can be useful with classification problems involving disjoint classes
            \item the log-likelihood cost function is often used with softmax activation
        \ei
    \end{enumerate}
    \item Log-likelihood criterion on stochastic neuron:
    \bi
        \item substituting (\ref{likelihood_notation}) into (\ref{llcf})
        \beq\label{lr_cf}\begin{split}
           c_{\bm{w}}=&  \log  \frac{1}{\theta(y_{\bm{w}}^T \bm{x})}\\
           = & \log (1+e^{-y_{\bm{w}}^T \bm{x}})
        \end{split}\eeq
        \item analytic solution to $\nabla C_\text{in}(\bm{w})=0$ is not feasible, but gradient descent algorithms can be used
        \item with classification problems, this cost metric converges faster than the mean square error metric
    \ei
\end{itemize}

\subsection{Update Rules\label{stochastic_sec}}
\begin{itemize}
  \item Perceptron Learning Algorithm (PLA):
    \bi
        \item pick any $(\bm{x},y)$ that is misclassified, and make the update
        \beq\begin{split}\label{pla}
            \bm{w}(t+1) &= \bm{w}(t) + y\bm{x} \Rightarrow \\
            y \, \bm{w}^T(t+1) \bm{x} &=  y \, \bm{w}^T(t) \bm{x} + y^2 \|x\|^2 \Rightarrow\\
            y \, \bm{w}^T(t+1) \bm{x} &> y \, \bm{w}^T(t) \bm{x}
        \end{split}\eeq
        \item since its a misclassified pair
        \beq\label{pla_error}
            y \, \bm{w}^T(t) \bm{x} < 0
        \eeq
        \item combining (\ref{pla}) with (\ref{pla_error}) implies $\bm{w}$ is moving in the right direction
        \item theorem: if the input data is linearly separable then the PLA will converge with $C_\text{in}=0$
    \ei
    \item Pocket algorithm:
    \bi
        \item when input data-set is not linearly separable, then the PLA is not stable
        \bi
            \item the problem of choosing $\bm{w}$ that minimizes $C_\text{in}$, i.e. error rate, is NP-hard
        \ei
        \item the \emph{pocket algorithm} is a variation of PLA, that keeps "in its pocket" the best $\bm{w}$
        \bi
            \item in other words, pick the best $\bm{w}$ rather than the last $\bm{w}$
            \item $C_\text{in}$ needs to be computed in every step
        \ei
    \ei
   \item Gradient descent:
    \bi
        \item gradient descent can be applied to a differentiable graph whether or not there is an analytic solution
        \item let $\nabla C_\text{in}(\bm{w}) = \nabla_\omega C_\text{in}(\bm{w})$ be the gradient of the cost function $C_\text{in}(\bm{w})$ with respect to $\bm{w}$
        \item gradient descent is based on the observation that  $C_\text{in}(\bm{w})$ decreases fastest in the direction of $- \nabla C_\text{in}(\bm{w})$
        \item define $\hat{\bm{v}}$ to be a unit vector in the direction of \emph{steepest descent}
        \beq\label{gd_unit}
            \hat{\bm{v}} \triangleq - \frac{\nabla C_\text{in}(\bm{w})}{\|\nabla C_\text{in}(\bm{w})\|}
        \eeq
        \item \emph{gradient descent} is an iterative solution, where at time $(t+1)$, the weights are adjusted as
        \beq\label{new_coef}\begin{split}
            \bm{w}(t+1) &= \bm{w}(t) + \delta \bm{w}\\
             &= \bm{w}(t) + \eta_t \hat{\bm{v}}
        \end{split} \eeq
        where $\eta_t$ is a hyper-parameter called the \emph{learning rate}
        \item to see why gradient descent proceeds in direction of $-\nabla C_\text{in}(\bm{w})$, apply Taylor expansion on $C_\text{in}(\bm{w})$
        \beq\label{gradient_descent_a}\begin{split}
             \Delta C_\text{in} &\triangleq C_\text{in}(\bm{w}(t)+\delta \bm{w}) - C_\text{in}(\bm{w}(t))\\
             &\approx \nabla C_\text{in}(\bm{w}(t)) \cdot  \delta \bm{w}\\
        \end{split}\eeq
        \item choosing $ \delta \bm{w} = \hat{\bm{v}}$ and $\eta_t = 1$
        \beq\label{gradient_descent_aa}\begin{split}
             \Delta C_\text{in} &= \nabla C_\text{in}(\bm{w}(t)) \cdot \hat{\bm{v}}\\
             &= - \nabla C_\text{in}(\bm{w}(t)) \cdot \frac{\nabla C_\text{in}(\bm{w}(t))}{\|\nabla C_\text{in}(\bm{w}(t))\|}\\
            &\geq -\| \nabla C_\text{in}(\bm{w}(t))  \|
        \end{split}\eeq
        \bi
            \item steepest descent is achieved when we have equality in (\ref{gradient_descent_a}), i.e. when
            \beq\label{gradient_descent_b}
                \delta \bm{w} = \hat{\bm{v}} = -\frac{\nabla C_\text{in}(\bm{w}(t))}{ \| \nabla C_\text{in}(\bm{w}(t))  \|}
            \eeq
        \ei
    \ei
    \item Local and global minima:
    \bi
        \item if the cost function is \emph{convex} then there is no \emph{local minima} but only a single \emph{global minimum}
        \item in practice, poor local minima are rarely a problem with large networks, or in higher dimensional space
        \item instead, the landscape is packed with a combinatorially large number of saddle points where the gradient is zero in many but not all dimensions
        \item practically, it does not much matter which of these saddle points the algorithm ends up being
    \ei
    \item Batch gradient descent:
    \bi
        \item it is desirable to get large steps when we are far from minimum, \& small steps when close to the minimum
        \item can choose learning rate $\eta_t$ to be proportional to the gradient
        \beq\label{lr_candidate}
            \eta_t = \eta \|\nabla C_\text{in}(\bm{w}(t))\|
        \eeq
        \item substituting (\ref{gradient_descent_b}, \ref{lr_candidate}) into $ \delta \bm{w}(t)$
        \beq\label{gradient_step}\begin{split}
             \delta \bm{w}(t) &= \eta_t \, \hat{\bm{v}}\\
             &= -\eta \|\nabla C_\text{in}(\bm{w}(t))\|\frac{\nabla C_\text{in}(\bm{w}(t))}{ \| \nabla C_\text{in}(\bm{w}(t))  \|} \\
             &= -\eta \nabla C_\text{in}(\bm{w}(t))\\
        \end{split}\eeq
        \item it follows that
        \beq\boxed{
         \bm{w}(t+1) = \bm{w}(t) -\eta \, \nabla C_\text{in}(\bm{w}(t))}
        \eeq
        \item this algorithm is sometimes referred to as  the \emph{batch gradient descent} or \emph{fixed learning rate gradient descent algorithm}
        \item note that a single update is generated from the entire data set
        \bi
            \item batch gradient descent can be very slow since the gradients for the whole dataset are calculated to perform just one update
            \item batch gradient descent are intractable for datasets that don't fit in memory
        \ei
    \ei
    \item Batch gradient descent for logistic regression:
    \bi
        \item the gradient of (\ref{lr_cf}) is
        \beq\label{lr_gradient}\begin{split}
             \nabla C_\text{in}(\bm{w}(t)) &=
             -\frac{1}{N} \sum_{n=1}^{N} \frac{y_n \bm{x}_ne^{-y_n\bm{w}^T \bm{x}_n}}{(1+e^{-y_n\bm{w}^T \bm{x}_n})}\\
             &=  -\frac{1}{N} \sum_{n=1}^{N} \frac{y_n \bm{x}_n}{1+e^{y_n\bm{w}^T \bm{x}_n}}\\
             &=  -\frac{1}{N} \sum_{n=1}^{N} y_n \bm{x}_n \theta(-y\bm{w}^T \bm{x}_n)
        \end{split}\eeq
        \item substituting (\ref{lr_gradient}) into (\ref{gradient_step})
        \beq\label{delta_w}
            \delta \bm{w}(t) =  \frac{1}{N} \sum_{n=1}^{N} y_n \bm{x}_n \theta(-y\bm{w}^T \bm{x}_n)
        \eeq
    \ei
    \item Stochastic gradient descent (SGD):
    \bi
        \item \emph{stochastic gradient descent} (SGD), is the sequential version of batch gradient descent, where an update is generated for each sample point $(\bm{x}_n,y_n)$
        \bi
            \item  $(\bm{x}_n,y_n)$ is picked uniformly at random, hence the name 'stochastic'
        \ei
        \item compared to batch gradient descent, SGD is simpler, faster and, due to randomization, is less prone to getting trapped in a local minima
        \item the rule of thumb is to use $\delta \approx 0.1$
        \item SGD complicates convergence to the exact minimum, as SGD will keep overshooting
    \ei
    \item Mini-batch gradient descent:
    \bi
        \item  \emph{mini-batch gradient descent} implies a single gradient update is generated per mini-batch of data $\{(\bm{x}_n,y_n)\}$
        \item each element of the mini-batch is picked uniformly at random
        \item mini-batch gradient descent takes the best of both SGD and batch gradient descent
        \bi
            \item compared to SGD, it reduces the variance of the parameter updates, which can lead to more stable convergence
        \ei
        \item common mini-batch sizes range between $50$ and $256$
        \item the term SGD usually is employed also when mini-batches are used
    \ei
    \item Learning rate with annealing:
    \bi
        \item with \emph{step decay}, the learning rate is reduced by some factor as a function of training iterations
        \item the \emph{exponential decay} and the $1/t$-\emph{decay} annealing functions are respectively
        \beq\label{anneal}\begin{split}
            \eta_t &= \eta_o \exp(-k t)\\
            \eta_t &= \frac{\eta_o}{1+k t}
        \end{split}\eeq
        where $\eta_0$, $k$ are hyper-parameters \& $t$ is the iteration number
    \ei
    \item Hessian technique:
    \bi
        \item applying Taylor's extension, as in (\ref{gradient_descent_a}), but including second order terms
        \beq\label{taylor_thm}\begin{split}
            \Delta C_\text{in}(\bm{w}+\delta \bm{w}) \approx \; \nabla C_\text{in} \cdot \delta \bm{w}\,+\,\frac{1}{2}\delta \bm{w}^T H \delta \bm{w}
        \end{split}\eeq
        where the \emph{Hessian matrix} $H$, is defined as
        \beq\label{hessian_def}
            H_{jk} \triangleq \frac{\partial^2 C_\text{in}}{\partial w_j \partial w_k}
        \eeq
        \item differentiating (\ref{taylor_thm}) with respect to $\delta \bm{w}$, and setting it to zero
        \beq\begin{split}
               0 &=  \nabla C_\text{in} +  H \delta \, \bm{w}\Rightarrow\\
               \delta \bm{w} &= - H^{-1} \nabla C
        \end{split}\eeq
        \item this approach to minimizing a cost function is known as the \emph{Hessian technique} or the \emph{Hessian optimization}
        \bi
            \item it is also referred to as the \emph{Newton's method}
        \ei
        \item Hessian methods converge in fewer steps than standard gradient descent
        \item furthermore, the Hessian approach could avoid many pathologies that can occur in gradient descent
        \item these benefits come at the expense of significant additional complexity
        \item two simplifications are BGFS and limited-memory BFGS (L-BFGS)
    \ei
    \item Momentum based gradient descent:
    \bi
        \item the momentum technique modifies gradient descent by introducing two new concepts:
        \begin{enumerate}
          \item Introducing the notion of "velocity" $\bm{v}_t$, for the parameters we're trying to optimize
          \bi
            \item the gradient acts to change the velocity, not (directly) the "position" $\bm{w}_t$, in much the same way as physical forces change the velocity, that only indirectly affect position
          \ei
          \item Introduces the notion of "friction" which tends to gradually reduce the velocity.
        \end{enumerate}
        \item the update rules are summarized as follows
        \beq\label{momentum}\begin{split}
               \bm{v}_t &= \mu \bm{v}_{t-1} - \eta \, \nabla C \\
               \bm{w}_t &= \bm{w}_{t-1} + \bm{v}_t
        \end{split}\eeq
        where $\mu$ is the \emph{momentum coefficient}, that controls the amount of damping or friction in the system
        \bi
            \item $\mu=0$ corresponds to the gradient descent method
            \item $0.5 \leq \mu \leq 0.9$ is common
        \ei
        \item the momentum technique is more practical than the Hessian technique
        \item the momentum technique often speeds up learning and is commonly used
    \ei
    \item Nesterov accelerated gradient:
    \bi
        \item \emph{Nesterov accelerated gradient} (NAG), is a momentum based technique, where the gradient is evaluated at $\bm{w} + \mu \bm{v}$ rather than at $\bm{w}$
    \ei
    \item Adagrad:
    \bi
        \item \emph{Adagrad} adapts the learning rate to the parameters
        \item so far, at time $t$, all the parameter $w_i$ used the same learning rate
        \item define $G_{t}$ to be a diagonal matrix, where $G_{t,ii}$ is the sum of the squares of the gradients w.r.t. $w_i$ up to time step $t$
        \item the update rule for Adagrad is
        \beq
            w_{t,i+1} = w_{t,i} - \frac{\eta}{\sqrt{G_{t,ii}+\epsilon}} \nabla C(w_i)
        \eeq
        where $\epsilon \sim 10^{-8}$ is a smoothing term that avoids division by zero
        \item compared to SGD, the learning rate is adaptively scaled by $\sqrt{G_{t,ii}+\epsilon}$
        \item without the square root operation, the algorithm performs much worse
        \item Adagrad eliminates the need to manually tune the learning rate
        \item most implementations use a default value of $\eta = 0.01$
        \item Adagrad also implicitly implements momentum based updates (why?)
        \item Adagrad's main weakness is its accumulation of the squared gradients in the denominator which eventually makes infinitesimally small
    \ei
    \item Adadelta:
    \bi
        \item \emph{Adadelta} is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate
        \item instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size $L$
    \ei
    \item  RMSProp:
    \bi
        \item  \emph{RMSProp} is an adaptive gradient technique where the learning rate is tailored for each parameter
        \item if the gradient at time $t$ is $\nabla_t C$, then the learning rate, at time $t$ is
        \beq\label{rmsprop}\begin{split}
            s_t   &= \alpha s_{t-1} + (1-\alpha)\|\nabla_t C\|^2\\
            \bm{w}_t &= \bm{w}_{t-1} + \eta \frac{\nabla_t C}{\sqrt{s_t}}
        \end{split}\eeq
    \ei
    \item Adam:
    \bi
        \item in the momentum technique (\ref{momentum}), $\bm{v}_t$ is linear in $\bm{g}_t$, whereas in RMSProp (\ref{rmsprop},  $s_t$ is quadratic in $\bm{g}_t$
        \item \emph{Adam} combines features from both of these techniques
        \beq\label{adam}\begin{split}
            \bm{v}_t &= \mu \bm{v}_{t-1} + (1-\mu)\nabla_t\\
            s_t   &= \alpha s_{t-1} + (1-\alpha)\|\nabla_t\|^2\\
            \bm{w}_t &= \bm{w}_{t-1} + \eta \frac{\bm{v}_t}{\sqrt{s_t}}
        \end{split}\eeq
        \item its is recommended to use $\mu = 0.9$, $\alpha = 0.999$
    \ei
\end{itemize}

\newpage
%--------------------- Generalization -------------------
\section{Generalization\label{gen_section}}
\begin{itemize}
    \item Generalization \& feasibility of learning:
    \bi
        \item \emph{generalization} is the capability of predicting a function for unobserved data
        \bi
            \item learning an arbitrary boolean function is not feasible by just observing a subset of the function
        \ei
        \item in machine learning, generalization is done based on probabilistic inference, Section  \ref{prob_sec}
        \item learning is feasible under the assumption that the training \& testing samples share the same distribution
    \ei
   \item Two goals of learning: choose hypothesis $ g \in \{h_i\}$:
    \begin{enumerate}
        \item to \emph{approximate}, or minimize $C(h)_\text{in}$ (\ref{in_sample}), and
        \item to generalize, i.e. from (\ref{in_sample}, \ref{out_sample}),
        \beq\label{generalization_def}
            C(g)_\text{out} \approx C(g)_\text{in}
        \eeq
        where $\approx$ means $C_\text{in}(h)=C_\text{out}(h)$ is \emph{probably approximately correct}, or P.A.C.
    \end{enumerate}
    \item Testing:
    \bi
        \item \emph{testing} is performed after a hypothesis $g$ is chosen
        \item testing is performed as a \emph{final} evaluation of accuracy
        \item testing is performed on a different data set than the training set
    \ei
    \item Overfitting:
    \bi
        \item \emph{overfitting} is the phenomenon where the in-sample data is fitted more than is warranted
        \item small $C_\text{in}$ is observed at the expense of $C_\text{out}$
        \item when overfitting, the network is learning about the peculiarities of the training set
        \item when overfitting, the network is
        \bi
            \item more memorizing than learning
            \item not learning to generalize from trend
        \ei
        \item a simple approach to contain overfitting is to \emph{early termination}, where training stops if no improvements are observed
    \ei
    \item Overfitting dependencies:
    \bi
        \item overfitting gets worst as
        \bi
            \item stochastic noise variance $\sigma_n^2$ increases
            \item target function complexity increases
            \bi
                \item for example if target function is modeled by a polynomial of higher degree
            \ei
            \item data size $N$ decreases
            \bi
                \item In many situations the data size should decide the complexity of the hypothesis family chosen, not the assumed complexity of the target function
            \ei
        \ei
    \ei
    \item Deterministic noise:
    \bi
        \item as defined in (\ref{bias_def}), deterministic noise is the part of the target function $f$ that $\mathcal{H}$ cannot capture
        \item deterministic noise is defined as
        \beq
            f(\bm{x}) - g(\bm{x})
        \eeq
        \item deterministic noise occurs when
        \bi
            \item $\mathcal{H}$ does not capture important aspects of $f$, i.e. $\mathcal{H}$ is oversimplified, or when
            \item target function complexity $Q_f$ is large and $N$ is not large enough
        \ei
        \item unlike stochastic noise, deterministic noise
        \bi
            \item depends on $\mathcal{H}$, and
            \item is repeatable
        \ei
        \item given $\mathcal{H}$, both noise sources have similar impact on the learning process
    \ei
    \item VC generalization bound:
    \bi
        \item VC bound provide a lower bound on $C_\text{out}(g) - C_\text{in}(g)$ as a function of sample size $N$
        \item see Appendix \ref{vc_section}, and (\ref{vc_gen_bound})
        \item in general, the VC bounds are very loose
    \ei
    \item Bias \& variance model:
    \bi
        \item an alternative method to decompose $C_\text{out}(g)$ is the \emph{bias \& variance method} reviewed in Appendix \ref{bv_section}
    \ei
    \item Two common approaches to address overfitting:
    \begin{enumerate}
      \item Regularization
      \item Validation
    \end{enumerate}
\end{itemize}


%---------------------- Regularization -------------------
\subsection{Regularization}
\begin{itemize}

    \item General remarks:
    \bi
        \item \emph{regularization} constrains the learning algorithm to improve out-of-sample error
        \bi
            \item constraining implies lowering $d_\text{VC}$
        \ei
        \item regularization is as much an art as it is science
        \bi
            \item there is no systematic understanding, only heuristics
        \ei
        \item in general, regularization reduces noise more than it limits the signal
    \ei
    \item The augmented metric:
    \bi
        \item the goal of regularization is to minimize an \emph{augmented metric}
        \beq\label{augmented_metric}
             C_\text{aug}(h) = C_\text{in}(h) + \Omega(h)
        \eeq
        rather than $C_\text{in}(h)$ alone, see (\ref{vc_omega})
        \item $\Omega(h)$ is the \emph{regularizer}, or the \emph{regularization term}
        \bi
            \item $\Omega(h)$ depends on $h$ rather than $\mathcal{H}$
            \item $\Omega(h)$ is a proxy for overfit penalty
        \ei
        \item the augmented metric $C_\text{aug}(h)$ is a proxy for $C_\text{out}(h)$
    \ei
    \item Regularization methods:
    \begin{enumerate}
        \item L2 regularization, or weight decay
        \bi
            \item other quadratic forms include
            \bi
                \item L2 regularization with importance factors
                \item Tikhonov regularization
            \ei
        \ei
      \item L1 regularization
      \item Soft weight elimination
      \item Soft targets to train
      \item Dropout
      \item Artificially expand the data set
    \end{enumerate}
    \item Hard \& soft order constraint:
    \bi
       \item for definiteness, consider a linear regression problem on the $\mathcal{Z}$ space (\ref{z_space}), of polynomials
       \item a polynomial hypothesis with lower degree (simpler) may result in better out-of-sample error
        \bi
            \item excluding all polynomials higher than a degree threshold is known as \emph{hard order constraint}
        \ei
        \item an example of a \emph{soft order constraint} is the constraint that polynomial coefficients satisfy
        \beq\label{consraint_energy}
            \sum_{q=0}^{Q} w_q^2 \leq C \Rightarrow \bm{w}^T\bm{w} \leq C
        \eeq
        \bi
            \item this is similar to FIR with normalized weight energy constraint
        \ei
    \ei
    \item L2 regularization:
    \bi
        \item the augmented error metric associated with (\ref{consraint_energy}) uses Lagrange multipliers to solve this constrained linear regression problem
        \beq\label{weight_decay}
            C_\text{aug}(\bm{w}) = C_\text{in}(\bm{w}) + \frac{\lambda}{N} \bm{w}^T \bm{w}
        \eeq
        \item with this augmented metric, the network prefers to learn small weights, all other things being equal
        \bi
            \item the smallness of the weights means that the behaviour of the network won't change too much if we change a few random inputs here and there
            \item this implies less noisy behaviour and simpler models
        \ei
        \item $\lambda$ is called the \emph{regularization parameter}
        \item minimizing (\ref{weight_decay}) is called \emph{L2 regularization}, or \emph{weight decay}, since taking the gradient of (\ref{weight_decay}) gives the additional term that is proportional to $\bm{w}$
        \item for the linear regression case, the resulting weight vector becomes (compare to (\ref{pseudo_inv}))
        \beq\label{w_reg}
            \bm{w}_\text{reg} =  (Z^T Z + \lambda I)^{-1}Z^T\bm{y}
        \eeq
        \item for gradient descent, the updated weights have the form
        \beq\label{l2_update}
            w \rightarrow \left(1 - \frac{\eta \, \lambda}{N} \right)w - \eta \frac{\partial e}{\partial w}
        \eeq
    \ei
    \item Weight decay of bias terms:
    \bi
        \item having a large bias doesn't make a neuron sensitive to its inputs
        \item no need to worry about large biases enabling our network to learn the noise in our training data
        \item allowing large biases gives our networks more flexibility in behaviour
        \bi
            \item in particular, large biases make it easier for neurons to saturate, which is sometimes desirable
        \ei
        \item for these reasons, usually bias terms are not regularized
    \ei
    \item Alternative weight constraints:
    \bi
        \item the constraint (\ref{consraint_energy}) can be generalized to,
        \beq
            \sum_{q=0}^{Q} \gamma_q w_q^2 \leq C
        \eeq
        where the \emph{importance factor} $\{\gamma_q\}$ are parameters that amplify/attenuate certain degrees
        \item in neural networks, different layers may get different importance factors
        \item the \emph{Tikhonov regularizer}
        \beq
            \bm{w}^T\Gamma^T \Gamma \bm{w} \leq C
        \eeq
        considers non-diagonal quadratic form, and captures relationships within $w_i$ by using matrix $\Gamma$
    \ei
    \item L1 regularization:
    \bi
         \item the augmented error metric of L2 regularization (\ref{weight_decay}), is modified to
        \beq\label{l1_ref}
            C_\text{aug}(\bm{w}) = C_\text{in}(\bm{w}) + \frac{\lambda}{N} \sum_n |w_n|
        \eeq
        \item the weights are then updated as
        \beq
            w \rightarrow w - \frac{\eta \, \lambda}{N} \text{sign}(w) - \eta \frac{\partial e}{\partial w}
        \eeq
        \item since derivative is not defined at $w=0$, apply the un-regularized rule for stochastic gradient, i.e. set $\text{sign}(0)=0$
        \item compared to (\ref{l2_update}), the decay in weight is constant, whereas in L2 regularization it is proportional $w$
        \item L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero
    \ei
    \item Hard \& soft weight elimination:
    \bi
        \item with \emph{hard weight elimination}, weights with values less than a threshold are eliminated
        \item with \emph{soft weight elimination}, the regularizer could be defined as
        \beq
            \Omega(\bm{w}) \triangleq \sum_{i,j,l} \frac{(w_{ij}^{(l))^2}}{\beta^2 + (w_{ij}^{(l)})^2}
        \eeq
        \bi
            \item for $\|\bm{w}^{(l)}\| \ll \beta$, numerator dominates and the regularizer reduces to weight decay (\ref{weight_decay})
            \item for  $\|\bm{w}^{(l)}\| \gg \beta$, $\Omega(\bm{w})$ is a constant and no updates are generated from the regularizer
            \item in summary, the regularizer attenuates small weights but not the large weights
        \ei
    \ei
    \item Dropout \& dropconnect:
    \bi
        \item \emph{dropout} modifies the network itself rather than modifying the cost metric
        \item more specifically, for each training example, forward propagation involves randomly deleting any hidden neuron with probability $p$
        \bi
            \item the error is then backpropagated only through the remaining activations
            \item $p= 0.5$ is common
        \ei
        \item we can think of dropout as a way of making sure that the model is robust to the loss of any individual piece of evidence
        \item dropout has been useful in training large, deep networks, where the problem of overfitting is often acute
        \item inverted dropout: after dropout the signal levels should be attenuated by $p$, (not $\sqrt{p}$?) since more inputs are contributing
        \item \emph{dropconnect} generalizes dropout by randomly dropping the weights rather than the activations
    \ei
    \item Soft targets:
    \bi
        \item \emph{soft target} means that, the label is a probability distribution rather than a one-hot vector
        \item when using a small network, use some combination of soft target, obtained from a larger circuit, and hard target to train data,
        \item soft targets are good regularizers
    \ei
    \item Practical rule for regularization:
    \bi
        \item constrain learning towards smoother hypothesis, since noise is high frequency
        \item in general smaller weights result in smoother hypothesis
    \ei
\end{itemize}

%---------------------- validation -------------------
\subsection{Validation\label{validation_section}}
\begin{itemize}
    \item The concept:
    \bi
        \item partition available training data, that was originally designated to training, to two parts
        \beq
            \mathcal{D} = \mathcal{D}_\text{val} \cup \mathcal{D}_\text{train}
        \eeq
        where the validation sample size is $|\mathcal{D}_\text{val}|=K$, and the re-allocated \emph{training} data size as $|\mathcal{D}_\text{train}|=N-K$
        \bi
            \item validation comes at the expense of training
            \item validation is used to optimize various  hyper-parameters
        \ei
        \item similar to the in-sample error (\ref{in_sample}), validation error is defined as
        \beq\label{val_error}
            C_\text{val}(h)  \triangleq \frac{1}{K}\sum_{n=1}^K e(h(\bm{x}_n),y_n)
        \eeq
        \item whereas regularization predicts $C_\text{out}$ indirectly th\-rough $\Omega(h)$, \emph{validation} directly measures $C_\text{out}$
    \ei
    \item A validation procedure:
    \bi
        \item generate a hypothesis $g^-$ from $N-K$ samples
        \item measure $C_\text{out}(g^-)$ from remaining $K$ samples
        \item generate a hypothesis $g$ from all $N$ samples
        \item report $g$ with measure $C_\text{out}(g^-)$
        \item as a rule of thumb, use $K \approx \frac{N}{5}$
    \ei
    \item Early stopping:
    \bi
        \item with \emph{early stopping}, validation data is used to monitor overfitting, picking the hypothesis where $C_\text{out}$ estimate is minimized
        \item validation then becomes an integral part of training
    \ei
    \item Training, testing and validating:
    \bi
        \item when training, $C_\text{in}$ is optimistically (deceptively) biased
        \item since testing does not alter the outcome of the learning process, $C_\text{test}$ is unbiased
        \item validating is similar to testing but can influence the outcome of the learning process because of feedback
        \bi
            \item then validation becomes optimistically biases as in training
            \item i.e. $C_\text{val}$ becomes a biased estimate of $C_\text{out}$
        \ei
        \item we want $C_\text{val}$ to be only slightly contaminated to be useful
    \ei
    \item Dependence of $g \in \mathcal{H}$ on data:
    \bi
        \item $g$ is a strong function of peculiarities of the training data
        \item $g$ should be a weak function of the peculiarities of validation data
        \item $g$ is independent of the peculiarities of the test data
    \ei
    \item $M$ models:
    \bi
        \item many validation approaches can be interpreted as choosing one of $M$ hypotheses, where each hypothesis was obtained from some model and training data
        \item then validation coincides with training, and bounds similar to (\ref{hoeffdings_inequality_b}, \ref{hoeffdings_inequality_c}) can be applied
    \ei
    \item Cross-validation:
    \bi
        \item so far the goal was to achieve
        \beq\label{e_approx}
            C_\text{out}(g) \approx  C_\text{out}(g^-) \approx  C_\text{val}(g^-)
        \eeq
        where for the first approximation to hold, $K$ needed to be small, while for the second approximation $K$ needed to be large
        \item in theory can run training $N$ separate times, where at each attempt the $n^\text{th}$ sample point is removed for validation purposes
        \item the \emph{cross-validation error} is then defined as the average of the $N$ validation errors
        \beq
            E_\text{cv} \triangleq \frac{1}{N} \sum_{n=1}^{N} e_n
        \eeq
        \item even though
        \bi
            \item each error term is from a different hypothesis, and
            \item the error terms are correlated
        \ei
        effectively, the error samples are almost uncorrelated
        \item in summary, effectively $K=1$ was used for first approximation of (\ref{e_approx}), and $K=N$ was used for second
    \ei
    \item $R$-fold validation:
    \bi
        \item cross validation, is not practical as described above, since it introduces $N$ times amplification in training time
        \item $R$-\emph{fold validation} implies partitioning sample data into $R$ subsets rather than $N$
        \item training sessions reduce from $N$ to $R$
        \item rule of thumb is to use $10$-fold cross-validation
    \ei
\end{itemize}

\newpage
\section{Feedforward Networks with Supervision\label{fnn_section}}
\begin{itemize}
    \item Historical overview:
    \bi
        \item \emph{cybernetics} in the $1940$s–$1960$s
        \bi
            \item biological learning
            \item perceptron
        \ei
        \item artificial neural networks (ANN) or \emph{connectionism}, in the $1980$s-$1990$s
        \bi
            \item parallel distributed processing (Rumelhart et al., $1986$ ; McClelland et al., $1995$)
            \item a large number of simple computational units can achieve intelligent behavior when networked together
            \item back propagation (Rumelhart et al., $1986$ )
        \ei
        \item \emph{deep learning} starting in $2006$, see Section \ref{deep_learning_sec}
        \bi
            \item pioneered by Geoﬀrey Hinton at University of Toronto, Yoshua Bengio at University of Montreal, and Yann LeCun at New York University
        \ei
    \ei
    \item Universality:
    \bi
        \item since a perceptron can model a NAND, a network of perceptrons can model any boolean function
        \item consider an arbitrary continuous function $f(\bm{x})$, with range within the activation function
        \item then for any $\epsilon>0$, there exist a NN $g(x)$, with a single hidden layer, such that
         \beq
            |g(\bm{x}) - f(\bm{x})| < \epsilon
         \eeq
         for all inputs $\bm{x}$
         \item the underlying concept is for the hidden layers to generate a basis function
         \item the result holds for any activation function $\theta(z)$, where $\theta(\pm \infty)$ are distinct and well-defined
         \item the question is not whether any particular function is computable, but rather what's a good way to compute the function
    \ei
    \item Feedforward networks:
    \bi
        \item feedforward  multilayer networks are the most popular ANNs
        \item such networks include an \emph{input layer}, an \emph{output layer}, and \emph{hidden layers}
        \item the role of the \emph{hidden layers} can be interpreted as distorting the input in a non-linear way so that categories become linearly separable by the last layer
    \ei
\end{itemize}



\subsection{Multilayer perceptrons}
\begin{itemize}
    \item General remarks:
    \bi
        \item \emph{multilayer perceptron}, or MLP, is a feedforward network where each layer is fully connected (FC) to the previous layer
        \item proposed in the 1980s
        \item the name is misnomer since the unit of MLP is not the perceptron
        \item MLPs use the supervised backpropagation algorithm to learn as will be discussed shortly
        \item MLPs do not learn well in the presence of many hidden layers
    \ei
    \item Notation:
    \bi
        \item a network is $L$ layers with $l \in \{0,1,\cdots, L\}$
        \bi
            \item the input layer corresponds to $l=0$ and does not count as a layer
            \item the output layer corresponds to $l=L$ which determines the value of the function
        \ei
        \item layer $l$ has \emph{dimension} $d^{(l)}$, which means it has $(d^{(l)}+1)$ \emph{nodes} (or \emph{units})
        \bi
            \item every layer has a \emph{bias node}, labelled by $0$
        \ei
        \item connections are between layers $(l-1)$ and $l$
        \bi
            \item $\bm{W}^{(l)} = \{w_{ij}^{(l)}\}$ is the weight from node $i$ in layer $(l-1)$ to node $j$ in layer $l$
            \item the signal and the output of node $j$ in layer $l$ are denoted respectively as $\bm{s}^{(l)}=\{s_j^{(l)}\}$ and  $\bm{x}^{(l)}=\{x_j^{(l)}\}$
        \ei
        \item hypothesis $h(\bm{w}) \in \mathcal{H}$ corresponds to hypothesis
        \beq
            \bm{w} \triangleq \{ \bm{W}^{(1)},  \bm{W}^{(2)},\cdots \bm{W}^{(L)} \}
        \eeq
    \ei
    \item Forward propagation:
    \bi
        \item load $\bm{x}^{(0)} = \bm{x}$
        \item for $l \in \{1, 2, \cdots, L\}$
        \beq\label{fw_prop}\begin{split}
            \bm{s}^{(l)} &= (\bm{W}^{(l)})^T \bm{x}^{(l-1)}\\
            \bm{x}^{(l)} &= \left[\begin{array}{c}
                               1 \\
                               \theta(\bm{s}^{(l)}) \\
                             \end{array}\right]\\
        \end{split}\eeq
        \item the final output is $h(\bm{x})= \bm{x}^{(L)}$
    \ei
    \item The sensitivity vector:
    \bi
        \item the \emph{sensitivity vector} $\bm{\delta}^{(l)}$ has components defined as
        \beq\label{sens_vector_def}\boxed{
             \bm{\delta}^{(l)}_j \triangleq \frac{\partial C_\text{in}}{\partial \bm{s}^{(l)}_j}}
        \eeq
        where the signal $\bm{s}^{(l)}_j$ is given by (\ref{fw_prop}), and the sample cost function $C_\text{in}$ is defined in (\ref{in_sample})
        \item interpretation
        \bi
            \item $\bm{\delta}^{(l)}_j$ is the 'signal error' in the $j^\text{th}$ unit
            \item $\|\bm{\delta}^{(l)}\|$ is a measure on how fast layer $l$ is learning
        \ei
        \item applying the chain rule
         \beq\label{sensitivity_vector}\begin{split}
           \bm{\delta}^{(l)}_j &= \frac{\partial C_\text{in}}{\partial \bm{x}^{(l)}_j} \cdot \frac{\partial \bm{x}^{(l)}_j}{\partial \bm{s}^{(l)}_j}\\
             &= \theta'(\bm{s}^{(l)}_j) \cdot \frac{\partial C_\text{in}}{\partial \bm{x}^{(l)}_j}
          \end{split}\eeq
          where the unit output $\bm{x}^{(l)}_j$ is is given by (\ref{fw_prop})
    \ei
    \item Recursive computation of of the sensitivity vector:
    \bi
        \item computing $\bm{\delta}^{(l)}$:
        \bi
            \item for last layer $L$, both terms in (\ref{sensitivity_vector}) are readily available
            \item for the other layers, a recursion on the sensitivity vectors can be obtained by running, a slightly modified version of the neural network backwards
        \ei
        \item since any component of $\bm{x}^{(l)}$ can affect $\bm{s}^{(l+1)}_k$,  $\partial C_\text{in} / \partial \bm{x}^{(l)}_j$ in (\ref{sensitivity_vector}) is computed as follows
        \beq\label{temp_bp}\begin{split}
            \frac{\partial C_\text{in}}{\partial \bm{x}^{(l)}_j} &= \sum_{k=1}^{d^{(l+1)}}
              \frac{\partial \bm{s}^{(l+1)}_k}{\partial \bm{x}^{(l)}_j}
             \frac{\partial C_\text{in}}{\partial \bm{s}^{(l+1)}_k}\\
             &=  \sum_{k=1}^{d^{(l+1)}} w_{jk}^{(l+1)}\bm{\delta}_k^{(l+1)}
        \end{split}\eeq
          \item substituting (\ref{temp_bp}) into  (\ref{sensitivity_vector})
        \beq\label{bp_eq}\boxed{
             \bm{\delta}^{(l)}_j =  \theta'(\bm{s}^{(l)}_j) \sum_{k=1}^{d^{(l+1)}} w_{jk}^{(l+1)}\bm{\delta}_k^{(l+1)}}
        \eeq
        \item in vector form
        \beq
            \bm{\delta}^{(l)} = \theta'(\bm{s}^{(l)}) \otimes [\bm{W}^{(l+1)}\bm{\delta}^{(l+1)}]_1^{d^{(l)}}
        \eeq
        where $\otimes$ denotes component-wise multiplication, known as \emph{Hadamard product} or \emph{Schur product}
    \ei
    \item The backpropagation algorithm:
    \bi
        \item for each batch $\{(\bm{x}_n,y_n)\}$,  and for all $i, j$ and $l$, it is desired to efficiently compute
        \beq\label{bp_gradient}
            \nabla C_\text{in}: \frac{\partial C_\text{in}}{\partial w_{ij}^{(l)}}
        \eeq
        \item \emph{backpropagation} is an algorithm that efficient computes (\ref{bp_gradient})
        \bi
             \item backpropagation is usually applied using the batch gradient descent update rule
        \ei
        \item to express (\ref{bp_gradient}) in terms of sensitivity vector $\bm{\delta}^{(l)}$, first apply the chain rule, then substitute the components with (\ref{fw_prop}, \ref{sens_vector_def})
        \beq\label{bp_update_rule}\begin{split}
            \frac{\partial C_\text{in}}{\partial w_{ij}^{(l)}} &=  \frac{\partial C_\text{in}}{\partial \bm{s}_j^{(l)}}\cdot \frac{\partial \bm{s}_j^{(l)}}{\partial w_{ij}^{(l)}}\\
            & =   \bm{\delta}^{(l)}_j \cdot \bm{x}_i^{(l-1)}\Rightarrow\\
            \frac{\partial C_\text{in}}{\partial \bm{W}^{(l)}} &=  \bm{x}^{(l-1)}\cdot \bm{\delta}^{(l)}
        \end{split}\eeq
        \bi
            \item recursively computation  of $\bm{\delta}^{(l)}$ (\ref{bp_eq}), enables the backpropagation algorithm
        \ei
        \item in forward propagation, the nonlinearity was the activation $\theta(.)$, whereas in backpropagation, it is multiplication by $\theta'(\bm{s}^{(l)}_l)$
        \item the backpropagation algorithm described above is SGD based, where the weights change after processing each data sample $(\bm{x}_i,y_i)$
        \item in practice, multiple $\bm{\delta}$s are computed before making a weight update
        \item the size of the samples grouped together is called the \emph{ mini batch size}
        \item more specifically, if the mini-batch size is $m$, then from (\ref{bp_update_rule})
        \beq
            \bm{W}^{(l)} \rightarrow  \bm{W}^{(l)} -  \frac{\eta}{m}\sum_x  \bm{x}^{(l-1)}(\bm{\delta}^{(l)}_x)^T
        \eeq
    \ei
\end{itemize}

%---------------------- Hyper-parameters -------------------
\subsection{Hyper-parameters}
\begin{itemize}
    \item General remarks:
    \bi
        \item \emph{hyper-parameters} are variables set before actually optimizing the weights of the NN
        \item finding a pseudo-optimal set of such parameters is a major challenge
        \item randomly generated parameters do in general better than grid search
    \ei
    \item An \emph{epoch} is a single forward and backward pass of the whole dataset
    \item List of \emph{hyper-parameters} include:
    \begin{enumerate}
            \item hidden layers, \& nodes per layer
            \item cost function
            \item activation
            \item epochs \& mini batch sizes
            \item learning rate
            \item generalization parameter
            \item weight initialization
    \end{enumerate}
    \item Weight initialization:
    \bi
        \item initialization is critical for deeper networks
        \item if initial weights are large then many nodes will saturate
        \item if initial weights are small then many node outputs stay close to zero, and weights do not get updated
        \item with fan-in $d$, it is recommended to generate weights using
        \beq\label{gauss_std}
            \mathcal{N}\left(0,\sqrt{\frac{\alpha}{d}}\right)
        \eeq
        where $\alpha$ is a hyper-parameter that is usually set to $\alpha = 1$ for $\tanh$ and $\alpha = 2$ for ReLU neuron models
        \item bias is not sensitive to initialization, so can be set to $0$, to Gaussian with standard deviation of one, or as in (\ref{gauss_std})
    \ei
\end{itemize}


\subsection{Visualization}
\begin{itemize}
    \item General remarks:
    \bi
        \item NN have high dimensionality and are hard to visualize
        \item in \emph{dimensionality reduction} high-dimensional data is mapped into lower dimensional data
        \bi
            \item one such method is PCA, see Appendix \ref{math_section}
            \item another approach is to find maps that are distance preserving
        \ei
        \item visualization addresses the problem of visualizing high dimensional data
        \item the space traversed by $\{\bm{x}_n\}$, at the input if a NN, is a very small subset of $\mathcal{R}^d$
    \ei
    \item Multidimensional scaling
    \bi
        \item \emph{multidimensional scaling}, or MDS, finds a distance preserving map by minimizing the cost function
        \beq\label{mds}
            \sum_{i,j} (d_{i,j}^* -d_{i,j})^2
        \eeq
        where $d_{i,j}^*$, and $d_{i,j}$ are the distances between $\bm{x}_i$ and $\bm{x}_j$ in the original and reduced spaces respectively
    \ei
    \item Sammon's mapping:
    \bi
        \item in Sammon’s mapping, more emphasis is given to preserving the distances between nearby points than between those which are far apart
        \item the associated cost function is a modification of (\ref{mds})
        \beq\label{sammon}
            \sum_{i,j} \frac{(d_{i,j}^* -d_{i,j})^2}{d_{i,j}^*}
        \eeq
    \ei
    \item Graph based visualization:
    \bi
        \item similar to Sammon's mapping, graph based visualization prioritizes local or nearby points
        \item in a \emph{nearest neighbor graph}, input vectors are the nodes and each node is connected to the three points that are closest to it in the original space
        \item the associated cost function is
        \beq
             \sum_{i,j} \frac{1}{d_{i,j}} + \frac{1}{2}\sum_{(i,j)\in E} (d_{i,j}- d_{i,j}^*)^2
        \eeq
    \ei
    \item t-SNE:
    \bi
        \item t-SNE stands for \emph{t-distributed stochastic neighbor embedding}
        \item t-SNE finds an embedding in two dimensions, such that locally, pairwise distances are conserved
        \item t-SNE tries preserve the topology of the data
        \bi
            \item for every point, t-SNE constructs a notion of which other points are its neighbors, trying to make all points have the same number of neighbors
            \item then it tries to embed them so that those points all have the same number of neighbors
            \item t-SNE is similar to the graph based visualization, but instead of just having points be neighbors or not neighbors, t-SNE has a continuous spectrum of having points be neighbors to different extents
        \ei
    \ei
    \item Deconv approach to visualization:
    \bi
        \item to visualize some neuron, forward propagate signals to that neuron
        \item set that neuron gradient to one and set all other neuron gradients in that layer to zero
        \item back propagate and reconstruct the input
        \item in (\ref{bp_eq}), set
        \beq
             S^{(l)}_j \triangleq  \sum_{k=1}^{d^{(l)}} w_{jk}^{(l)}\bm{\delta}_k^{(l)}
        \eeq
        \item with \emph{guided backpropagation}, backpropagation (\ref{bp_eq}), is modified to
        \beq
             \bm{\delta}^{(l)}_j =  \theta'(\bm{s}^{(l)}) \, (S^{(l+1)}_j > 0) \, S^{(l+1)}_j
        \eeq
        i.e. only positive gradients are passed on to the previous layer, and so only neurons that positively help activation are incorporated
        \item with \emph{backward deconvnet}
        \beq
             \bm{\delta}^{(l)}_j =  (S^{(l+1)}_j > 0) \, S^{(l+1)}_j
        \eeq
    \ei
     \item Examples of visualization:
    \bi
        \item at first, or lowest, layer can use weights for visualization
        \bi
            \item the filters look like Gabor filters, see Section \ref{math_section}
        \ei
        \item at the top layers, check what set of stimuli does a neuron responds to
        \item in an \emph{occlusion experiment} the classification decision is monitored as a squared section of the input is zeroed out and moved around
    \ei
    \item Optimization over image approaches:
    \bi
        \item the goal is to find an image that maximizes some score,
        \item start by feeding all zeros
        \item  set the gradient of the scores vector to be $[0,0,....1,....,0]$, then back-propagate to image
        \item update image
        \bi
            \item can blur it, or zero pixels with small norm
        \ei
        \item forward propagate
        \item keep iterating
    \ei
    \item Adversarial examples:
    \bi
        \item can take an arbitrary image, can make targeted but minor changes to the input so that it will be classified to any other false object
        \item presently this is a concern
    \ei
\end{itemize}


%---------------------- Deep Learning -------------------
\subsection{Deep learning\label{deep_learning_sec}}
\begin{itemize}
    \item Deep neural networks \& deep learning:
    \bi
        \item the quintessential example of \emph{deep neural networks} (DNN) is the feedforward ANN
        \bi
            \item DNN includes more hidden layers, and each hidden layer in general includes more neurons
        \ei
        \item in recent years, \emph{deep learning} (DL) has become the most popular approach to developing AI
    \ei
    \item Shallow versus deep NNs:
    \bi
        \item even though any function could be computed using a shallow network, it may not be a good choice
        \item with DNN, it is possible to construct a solution through multiple layers of abstraction
        \item for some functions very shallow circuits require exponentially more elements to compute a problem than do DNNs
    \ei
    \item Unstable gradient as main challenge to DNNs:
    \bi
        \item in general, the gradient in DNNs is unstable, tending to either explode or vanish in earlier layers
        \bi
            \item this is due to the fact that the updated quantity is the product of terms from all the later layers
        \ei
        \item when the gradient tends to get smaller as we move backward through the hidden layers, then neurons in the earlier layers learn slower than neurons in later layers
        \bi
            \item this is known as the \emph{vanishing gradient problem}
            \item it occurs due to product of small quantities
        \ei
        \item in other instances, the gradient gets larger in earlier layers
        \bi
            \item this is known as \emph{the exploding gradient problem}
            \item it occurs if the weights in the product are of large quantities
        \ei
    \ei
    \item The traditional ML approach
    \bi
        \item to address unstable gradient problem, traditional ML approach was based on first developing hand-tuned feature extractors
        \item kernel methods, Appendix \ref{svm_sec}, can handle non-linearities but do not generalize well in practice
        \item hand tuned feature extractors are not robust
        \item it is difficult to design reliable feature extractors
    \ei
    \item Deep belief networks:
    \bi
        \item \emph{deep belief networks}, or DBNs, are a class of DNN that use RBMs (\ref{rbm}), to train the initial layers
        \item they were the first generation of DNNs to address unstable gradient problem
    \ei
    \item Hierarchical approach in DL:
    \bi
        \item DL methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level
        \item deep layers allow for hierarchical feature representations
        \item with the composition of enough such transformations, very complex functions can be learned
        \item each module in the stack transforms its input to increase both the selectivity and the invariance of the representation
        \item for classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations
    \ei
    \item Enablers of DNN:
    \begin{enumerate}
        \item Diverse data availability
        \bi
            \item availability of digitized data due to internet, cloud etc.

            \item techniques to artificially expand the training data, see Section \ref{data_set_section}
        \ei
        \item Improvements in learning algorithms
        \bi
            \item introduction of new activations such as ReLU (\ref{relu})
            \item regularization algorithms such as dropout
            \item automatic feature extractors using convolutional \& pooling layers
        \ei
        \item Hardware acceleration
        \bi
            \item Moore's law
            \item Graphic Processing Units (GPU), see Section \ref{tools_section}
        \ei
    \end{enumerate}
\end{itemize}

%---------------------- Convolutional Neural Networks -------------------
\subsection{Convolutional, inception \& residual networks}
\begin{itemize}
    \item Convolutional neural network:
    \bi
        \item a \emph{convolutional neural network}, or CNN, is a class within DNN
        \item the \emph{Neocognitron} (Fukushima, $1980$) introduced a powerful model architecture for processing images, inspired by the structure of the mammalian visual system
        \item Neocognitron is the basis for CNN network as proposed by LeCun et al., in $1998$ used in vision tasks
    \ei
    \item Network size concerns \& remedies:
    \bi
        \item if size of data set is fixed, larger network usually means more parameters, making it prone to overfitting
        \item larger networks increase computational resources
        \item to remedy these two issues, additional constraints are introduced into the network such as regularization, sparsity, or structure
        \item sparsity replaces the fully connected layers by sparser ones
        \bi
            \item  today’s computing infrastructures are inefficient when it comes to numerical calculation on non-uniform sparse data structures
        \ei
        \item convolutional neural networks introduce some form of constraint to the network
    \ei
    \item CNNs use three ideas:
    \begin{enumerate}
      \item Local receptive fields
      \item Shared weights
      \item Pooling
    \end{enumerate}
    \item Local receptive fields \& stride length:
    \bi
        \item usually input data is represented as a rectangular format, rather than linear
        \item in ANN there is full connectivity between layers $(l-1)$ and $l$
        \item in a CNN the connections are in a small, localized regions of the input image
        \item the region in the input image that is connected to a hidden neuron is called the \emph{local receptive field} or a \emph{patch}
        \item different hidden neurons cover different local receptive fields
        \item adjacent local receptive fields are separated by one or more pixels, known as the \emph{stride length}
    \ei
    \item Shared weights:
    \bi
        \item the set of hidden units that cover all the receptive fields, all share the same weights and biases
        \bi
            \item the \emph{shared weights} and \emph{bias} are often said to define a \emph{kernel} or \emph{filter bank}
        \ei
        \item every element of this set is extracting the same feature, but at a different position
        \bi
            \item  the map from the input layer to the hidden layer is sometimes called a \emph{feature map}
        \ei
        \item the weights and biases learned for a given output layer are shared across all patches in a given input layer
    \ei
    \item Filter Depth:
    \bi
      \item in many visual applications, data at each layer is presented in $3$-dimensions $(H \times W \times F)$
      \bi
            \item the amount of filters $F$, in a convolutional layer is called the \emph{filter depth}
            \item for input layer, the feature set could be the three primary colors
            \item for hidden layers, multiple feature maps are usually extracted using multiple filter banks
      \ei
      \item for example, consider the convolutional layer connection
      \beq\label{filter_size}\begin{CD}
            (H \times W \times F_1) @>F_2 \times (3\times 3 \times F_1)>>  (H \times W \times F_2)
      \end{CD}\eeq
      where each filter has $3\times 3 \times F_1$ local receptive field, \& for each location on the output layer there are $F_2$ filters
      \item $F_1$ \& $F_2$ refer to the feature sets at first and second layers
    \ei
    \item Convolutional layer:
    \bi
        \item a hidden layer with shared filter bank is called  a \emph{convolutional} layer since the output is the convolution of a rectangular window and the input
        \item it is common to zero pad the border if hidden layer sizes need to be preserved,
        \item convolutional filters
        \bi
            \item provides translation invariance of images
            \item dramatically reduces the number of parameters, compared to full FC layers
        \ei
    \ei
    \item On filter size trends: $(1 \times 1)$ convolutions
    \bi
        \item recently, multiple layers of smaller sized filters are chosen over a single layer of larger sized filters
        \bi
            \item $(1 \times 1 \times F)$ layers
            \item such architectures, in general, use less memory, are more computationally efficient, \& provide more non-linearities
        \ei
        \item a FC layer with $n$ output elements is equivalent to a  $(1 \times 1 \times n)$ layer
        \item can factor $(n \times n)$ convolutions into $(1 \times n)$  \& $(n \times 1)$
    \ei
    \item Example on filter size tradeoffs: a bottleneck architecture
    \bi
        \item consider the connection architecture of (\ref{filter_size}) with $F_1=F_2=F$
        \item this connection architecture can be replaced by a \emph{bottleneck architecture}, where
        \beq\begin{CD}
             (H \times W \times F) @>\tfrac{F}{2} \times (1,1,F)>>  (H \times W \times \tfrac{F}{2}) @>F/2 \times (3,3,\tfrac{F}{2})>>  (H \times W \times \tfrac{F}{2}) @>F \times (1,1,\tfrac{F}{2})>>  (H \times W \times F)
        \end{CD}\eeq
        \bi
            \item first, the layer size is reduced to $(H \times W \times F/2)$ by using $F/2$ $(1 \times 1 \times F)$ filters per position
            \item then another layer of same size is generated by using  $F/2$ $(3 \times 3 \times F/2)$ filters
            \item finally, original layer size $(H \times W \times F)$  is restored using $F$  $(1 \times 1 \times F/2)$ filters
        \ei
    \ei
    \item Transposed convolution layer: asd
    \bi
        \item \emph{transposed convolution} helps in upsampling the previous layer to a higher resolution or dimension, see Section \ref{vision_nn_section}
        \item the term transpose means to transfer to a different place or context
        \bi
            \item use a transposed convolution to transfer patches of data onto a sparse matrix
            \item then can fill the sparse area of the matrix based on the transferred information
        \ei
    \ei
    \item Super-parameters needed for convolutional layer:
    \begin{enumerate}
      \item Features or filer banks
      \bi
        \item choose a power of $2$, like $64$
      \ei
      \item Filter size, or local receptive fields
      \bi
            \item use smaller sizes at lower layers and larger sizes for higher layers
      \ei
      \item The stride length
      \bi
        \item choose $1$ or $2$
      \ei
      \item Zero padding
    \end{enumerate}
    \item Pooling layer:
    \bi
        \item a \emph{pooling layer} is usually used after convolutional layers
        \item a pooling layer simplifies, or condenses, the output information from the convolutional layer
        \bi
            \item the role of the pooling layer is to merge semantically similar features into one
            \item each feature map is pooled separately
        \ei
        \item conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements
         \item pooling reduces the dimension of the representation and creates an invariance to small shifts and distortions
         \bi
            \item pooling reduces the number of parameters needed in later layers
         \ei
         \item common pooling strategies do not introduce new parameters but do introduce new hyper-parameters, including\emph{ pooling size} and \emph{pooling stride}
         \item  the pooling operation is applied individually for each depth slice
         \bi
             \item for a pooling layer the output depth is the same as the input depth
         \ei
         \item $(2 \times 2)$ pooling with stride of $2$  is common
    \ei
    \item Pooling methods:
    \bi
        \item with \emph{max-pooling}, a unit simply outputs the maximum activation in an input region
        \beq
            y = \max_i(X_i)
        \eeq
        \item with \emph{average pooling}, the average signal is picked
        \beq
            y = \text{mean}_i(X_i)
        \eeq
        \item with \emph{L2 pooling}, the square root of the sum of the squares of the activations are computed
        \beq
            y = \sqrt{\sum_i X_i^2}
        \eeq
    \ei
    \item Hierarchical approach to the softmax output layer:
    \bi
        \item when there are large number of classes, the softmax output layer becomes complex
        \item a hierarchical approach introduces classes $c_t$, where multiple classes would belong to class $c_t$
        \item the conditional probably distribution can then be factored as
        \beq
            p(o_t | \text{history}) =  p(c_t | \text{history}) \, p(o_t|c_t)
        \eeq
    \ei
    \item Inception network:
    \bi
        \item \emph{inception} is computationally efficient  architecture
        \item inception-v1, see \cite{szegedy2015a}:
        \bi
            \item an inception module or layer, is constructed by the concatenation of various filters such as pooling operations \& convolutional filters of different width
            \item an inception layer also uses filter size reduction as discussed in (\ref{filter_size})
            \item an \emph{inception network} is a network consisting of Inception modules, stacked upon each other, with occasional max-pooling layers with stride $2$ to halve the resolution of the grid
        \ei
        \item inception-v2 introduced batch normalization (\ref{bn})
        \item inception-v4 includes more diverse types of inception \& reduction layers \cite{szegedy2016}
    \ei
    \item Residual connections:
    \bi
        \item introduced by He et al. in $2015$
        \item residual connection utilize additive merging of signals from different layers
        \item if $x$ is the ReLU output from previous layer, then residual connection computes the next layer output as
        \beq\label{resnet_update}
            y = \text{ReLU}[\text{Conv}(\text{Conv}(x))+x]
        \eeq
    \ei
    \item Trends:
    \bi
        \item smaller filters
        \item deeper architectures
        \item possibly dropping pooling
        \bi
            \item recent datasets are so big and complex we are more concerned about under-fitting
            \item dropout is a better regularizer
            \item pooling results in a loss of information
        \ei
        \item dropping some or all FC layers
        \item special layers such as residual connections, inception layers or some combinations of the two
    \ei
  \end{itemize}


\subsection{Transfer Learning\label{tl_sect}}
\begin{itemize}
    \item \emph{Transfer learning} involves taking a pre-trained neural network and re-adapting it to a new, different data set
    \item Approaches to transfer learning:
    \begin{enumerate}
        \item new data set is small, new data is similar to original training data
        \bi
            \item retrain only top FC layer
        \ei
        \item new data set is small, new data is different from original training data
        \bi
            \item replace all FC layers and last convolutional layer with a single FC layer
            \item this is known as \emph{feature extraction}
        \ei
        \item new data set is large, new data is similar to original training data
        \bi
            \item remove the last FC layer and replace with a layer matching the number of classes in the new data set
            \item initialize the rest of the weights using the pre-trained weights
            \item re-train the entire neural network
            \item this is known as \emph{finetuning}
        \ei
        \item new data set is large, new data is different from original training data
        \bi
            \item remove the last FC layer and replace with a layer matching the number of classes in the new data set
                \item retrain the network from scratch with randomly initialized weights
        \ei
    \end{enumerate}
    \item With modest training data, can retrain both middle and upper layers
    \bi
        \item re-learn middle layers with $1\%$ learning rate of original network
        \item re-learn upper layers with $10\%$ learning rate of original network
    \ei
    \item Popular networks to start from:
    \begin{enumerate}
        \item LeNet \cite{lecun1998}
        \bi
            \item the first successful applications of CNN, in $1998$
            \item best known for reading zip codes, digits, etc.
        \ei
        \item AlexNet \cite{krizhevski2012}
        \bi
            \item popularized CNN in computer vision by winning ImageNet $2012$, by a significant margin
            \item compared to LeNet, its deeper, bigger, \& featured convolutional layers stacked on top of each other
            \item used ReLU activation and dropout to avoid overfitting
            \item $8$ layers, $5$ convolution layers followed by $3$ FC layers
        \ei
        \item ZFNet \cite{zf2013}
        \bi
            \item winner of ImageNet $2013$
            \item compared to AlexNet, the size of the middle convolutional layers were expanded and the stride and filter size on the first layer were reduced
        \ei
        \item GoogLeNet \cite{szegedy2015a}
        \bi
            \item winner of ImageNet $2014$
            \item the \emph{GoogLeNet} name refers to a particular incarnation of the inception architecture used in the submission for the ILSVRC $2014$ competition
            \item it is known to be fast and has potential for real-time applications
            \item $22$ layers with parameters, i.e. without pooling
        \ei
        \item VGGNet \cite{vggnet2014}
        \bi
            \item from Visual Geometry Group at Oxford University
            \item the runner-up in ImageNet $2014$
            \item it features a homogeneous architecture that only performs $(3 \times 3)$ convolutions with $(2 \times 2)$ pooling layers, followed by three layers of FC layers
            \item known to be a flexible network
            \item but it is expensive to evaluate and uses a lot more memory and parameters (140M)
            \item VGG16 is the $16$ layer version, while VGG19 has $19$ layers
        \ei
        \item ResNet \cite{he2016a}
        \bi
            \item winner of ImageNet $2015$
            \item \emph{ResNet}, short for residual network
            \item ResNet is an ultra-deep network with $152$ layers
            \item ResNet uses residual connections  (\ref{resnet_update}), batch normalization (\ref{bn}), and $(1 \times 1)$ convolutional filters (\ref{filter_size})
            \bi
                \item ResNet does not include FC fully layers at the top of the network
            \ei
            \item ResNet is currently the state of the art of CNNs, and are the default choice for using CNNs in practice
        \ei
    \end{enumerate}
\end{itemize}

\newpage
%---------------------- RNN -------------------
\section{Recurrent Neural Networks with Supervision\label{rnn_section}}
\begin{itemize}
    \item Overview:
    \bi
        \item \emph{Recurrent Neural Networks} (RNN) entail feedback,
        \item RNN networks are effective for
        \bi
            \item visual attention systems work sequentially on an input,
            \item systems that receive a sequence of inputs, such as sentiment analysis,
            \item systems that produce a sequence of outputs, for example, captioning an image,
            \item systems with both sequential inputs and outputs such as machine translation,
        \ei
        \item RNNs can be viewed as programs, or state machine,
        \item the final state vector of an RNN's hidden units may represent the thought expressed by the sentence.
    \ei
    \item RNN model:
    \bi
        \item if $\bm{x}$ is the input, and $\bm{h}$ is output of the hidden layer, then
        \beq
            \bm{h}_{t}= f\left(\bm{x}_t,\bm{h}_{t-1} \right),
        \eeq
        \item if hidden layers form a single layer,
        \beq
               \bm{h}_{t}= \theta\left( W^{(x)} \bm{x}_t + W\, \bm{h}_{t-1} \right),
        \eeq
        \item alternatively, if $\bm{h}$ is the signal level at the hidden layer, then the an RNN can be modeled as
        \beq\label{rnn_model}\begin{split}
            \bm{h}_{t}&= W^{(x)} \bm{x}_t + W\, \theta(\bm{h}_{t-1}),
        \end{split}\eeq
        \item then the output is of the form
        \beq\begin{split}
             \hat{y}_{t} &= W^{(s)} \, \theta(\bm{h}_{t}).
        \end{split}\eeq
    \ei
    \item Deep RNN:
    \bi
        \item stack multiple RNN layers, where the hidden state of one RNN is the input of the next RNN,
        \item  there are multiple hidden layers between inputs and outputs.
    \ei
    \item Bidirectional RNN:
    \bi
        \item causal is forward, anti-causal in backwards, like BCJR,
        \item the output is a function of the concatenation of the two hidden layer outputs.
    \ei
    \item Deep bidirectional RNN:
    \bi
        \item a deep and bidirectional extensions of an RNN can be combined to create a deep bidirectional RNN.
    \ei
    \item Encoder and decoders:
    \bi
        \item for some applications, such and language translation, can think of the system model of doing two tasks, encoding and decoding
        \item in the basic RNN model (\ref{rnn_model})
        \bi
            \item encoding occurs while there is an incoming stream of inputs $\bm{x}_t$,
            \item decoding occurs when there is an output $\hat{y}_t$,
        \ei
        \item assume decoder starts after encoder ends.
    \ei
    \item Extensions to RNN for encoders/decoders:
    \bi
        \item this basic model (\ref{rnn_model}), can be extended in multiple ways:
        \bi
            \item use different weights $W'$, for decoding,
            \beq\label{rnn_model_b}\begin{split}
                \bm{h}_{D,t}&=  W' \, \theta(\bm{h}_{t-1}),\\
            \end{split}\eeq
            \item remember last state of the encoder $\bm{h}_T$,
            \beq\label{rnn_model_c}\begin{split}
                \bm{h}_{D,t}&=  W' \, \theta(\bm{h}_{t-1})  + W^T\theta(\bm{h}_T ) \\
            \end{split}\eeq
            \item include the previous output $\hat{y}_{t-1}$,
            \beq\label{rnn_model_d}\begin{split}
                \bm{h}_{D,t}&=  W' \, \theta(\bm{h}_{t-1})  + W^T\theta(\bm{h}_T ) + W^y \hat{y}_{t-1}.
            \end{split}\eeq
        \ei
    \ei
    \item Vanishing / exploding problem in RNN:
    \bi
        \item denote the cost function at time $t$ as $E_t$,
        \item the derivative of $E_t$ w.r.t. $W$ can be expressed as,
        \beq\label{rnn_grad}\begin{split}
               \frac{\partial E_t}{\partial W} &=
               \sum_{\tau=1}^{t} \frac{\partial E_t}{\partial y_t} \frac{\partial y_t}{\partial \bm{h}_t} \frac{\partial \bm{h}_t}{\partial \bm{h}_\tau}\frac{\partial \bm{h}_\tau}{\partial W} \\
            \end{split}
        \eeq
        \item using (\ref{rnn_model}), expand one of Jacobian terms in (\ref{rnn_grad}),
        \beq\begin{split}
            \frac{\partial \bm{h}_t}{\partial \bm{h}_\tau} &= \prod_{j=\tau+1}^{t}\frac{\partial \bm{h}_j}{\partial \bm{h}_{j-1}}\\
            &=  \prod_{j=\tau+1}^{t} W^T \text{diag}[\theta'(\bm{h}_{j-1})],
        \end{split}\eeq
        \item then the gradient becomes the product of all these Jacobian matrices which becomes very small or very large quickly,
        \item for exploding problem can saturate the gradient,
        \item for vanishing problem initialize $W=I$ and use ReLU.
    \ei
    \item Learning with long term memory:
    \bi
        \item theoretical and empirical evidence shows that it is difficult to learn to store information for long sequences,
        \item three approaches include,
        \bi
            \item gating techniques,
            \item \emph{Neural Turing Machine}, where the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to,
            \item \emph{memory networks}, where a regular network is augmented by a kind of associative memory.
        \ei
    \ei
    \item Gating:
    \bi
        \item \emph{gating} is a technique that helps the network to decide when to forget or remember an input,
        \item gating techniques use RNN where each hidden layer is more complex,
        \item two popular gating techniques are GRU and LSTM,
    \ei
    \item GRU:
    \bi
        \item the ideas of \emph{Gated Recurrent Units} (GRU) are,
        \bi
            \item to keeps around memories to capture long distance dependencies,
            \item allow error messages to flow at different strengths depending on inputs,
        \ei
        \item the GRU first computes two gates, \emph{update gate} $z_t$, and the \emph{reset gate} $r_t$,
        \beq\begin{split}
            z_t &\triangleq \sigma\left(U^{(z)}x_t + W^{(z)}h_{t-1}\right),\\
            r_t &\triangleq \sigma\left(U^{(r)}x_t + W^{(r)}h_{t-1}\right),
        \end{split}\eeq
        \bi
            \item $\sigma$ is the sigmoid activation (\ref{sigmoid}),
            \item $h_t$ is the output of the activation,
            \item think of these two gates as a special hidden layers,
        \ei
        \item intermediate memory content is
        \beq
            \tilde{h}_t \triangleq \tanh( W x_t + r_t \circ U h_{t-1}),
        \eeq
        where $\circ$ is the Hadamard multiplication,
        \item the hidden layer is updated as
        \beq
            h_t \triangleq z_t \circ h_{t-1} + (1-z_t)\circ \tilde{h}_t.
        \eeq
    \ei
    \item LSTM:
    \bi
        \item \emph{Long short-term memory} (LSTM) \cite{lstm1997}, is a popular and powerful tool,
        \item LSTM is similar to GRU but three gates are used rather than two: input gate $i_t$, forget gate $f_t$, and output gate $o_t$,
        \beq\begin{split}
            i_t & \triangleq \sigma \left( W^{(i)} x_t + U^{(i)}h_{t-1} \right),\\
            f_t & \triangleq \sigma \left( W^{(f)} x_t + U^{(f)}h_{t-1} \right),\\
            o_t & \triangleq \sigma \left( W^{(o)} x_t + U^{(o)}h_{t-1} \right),\\
        \end{split}\eeq
        \item the intermediate memory cell $\tilde{c}_t$, the final memory cell $c_t$, and the final hidden state $h_t$, are respectively,
        \beq\begin{split}
            \tilde{c}_t &\triangleq \tanh\left( W^{(c)} x_t +  U^{(c)} h_{t-1}\right),\\
            c_t &\triangleq f_t \circ c_{t-1} + i_t \circ \tilde{c}_t,\\
            h_t &\triangleq o_t \circ \tanh(c_t),
        \end{split}\eeq
        \item $c_t$ introduces additive interaction, similar to a ResNet,
        \item these additions also help backpropagation,
        \item multiple LSTM's are usually stacked up to generate a deep LSTM.
    \ei
    \item Recursive neural networks are covered in Section \ref{nlp_section}.
\end{itemize}

\newpage
%----------------------------------------------------------
\section{Unsupervised Learning\label{unsupervised_section}}
\begin{itemize}
    \item Unsupervised learning:
    \bi
        \item the goal of \emph{unsupervised learning} is to learn some structure of the data, without labels or teacher,
        \item still mostly a research topic,
        \item examples include
        \bi
            \item clustering,
            \bi
                \item $k$-means clustering is unsupervised clustering algorithm, not based in NN, see Appendix \ref{math_section}.
            \ei
            \item dimensionality reduction \& feature learning,
            \bi
                \item see autoencoders below,
            \ei
            \item generative modeling,
            \bi
                \item see variational autoencoders below.
            \ei
        \ei
    \ei
    \item Hebbian rule:
    \bi
        \item neurons that fire together, wire together,
        \item more specifically, \emph{Hebbian rule}, is a simple, unsupervised learning algorithm \cite{hebb1949}, where the weight $w_{ij}$ connecting neurons $x_i$ to $x_j$ is updated proportional to
        \beq\label{hebbian_learning}
            \frac{d w_{ij}}{dt} \propto \text{correlation}(x_i x_j).
        \eeq
    \ei
    \item Energy-based models:
    \bi
        \item \emph{energy-based models} associate a scalar energy to each configuration of the variables of interest,
        \item learning corresponds to modifying the energy function so that its shape has desirable properties,
        \bi
            \item  for example, we would like desirable configurations to have low energy,
        \ei
        \item Hopfield networks and Boltzmann machines are examples of energy-based models.
    \ei
    \item Hopfield networks:
    \bi
        \item a \emph{Hopfield network} is an undirected graph with only visible nodes,
        \item a Hopfield model is an energy-based model, where the goal is to make the $N$ samples $\{\bm{x}_i\}$, the stable states of the network,
        \item the neurons can be activated synchronously (simultaneously), or asynchronously (one at a time),
        \item applications:
        \bi
            \item associative memories, or content addressable memories,
            \item optimization problems.
        \ei
    \ei
    \item Hopfield network learning:
    \bi
        \item let $x_i^{(n)} \in [-1, 1]$, be the activation output,
        \item the weights are set based on Hebbian learning (\ref{hebbian_learning}),
        \beq
            w_{ij} \sim \sum_{n \leq N} x_i^{(n)} x_j^{(n)},
        \eeq
        \item the resulting learning algorithm is unsupervised,
        \item the weights are symmetric, i.e.,  $w_{ij}=w_{ji}$,
        \item there are no self connections, i.e., set $w_{ii}=0$.
    \ei
    \item Boltzmann machines \& RBM:
    \bi
        \item a \emph{Boltzmann machine} differs from a Hopfield network in two ways,
        \bi
            \item the Boltzmann machine also allows hidden nodes,
            \item the Boltzmann machine uses stochastic neuron, with probabilistic firing mechanism,
        \ei
        \item a \emph{restricted Boltzmann machine} (RBM) is a Boltzmann machine with a bipartite structure
        \beq\label{rbm}
            \bm{x} \leftrightarrow \bm{h}
        \eeq
        where the hidden nodes constitute one layer and visible nodes the other,
        \bi
            \item there are no connections within a layer.
        \ei
    \ei
    \item Autoencoders:
    \bi
        \item an \emph{autoencoder} is trained to encode the input $\bm{x}$ into some representation $c(\bm{x})$, so that $\bm{x}$ can be reconstructed from $c(\bm{x})$,
        \beq\label{ae}\begin{CD}
            \bm{x} @>\text{encoder}>> c(\bm{x}) @>\text{decoder}>> \bm{x}
        \end{CD}\eeq
        \bi
            \item autoencoders are symmetric in the sense that the encoder and a decoder could share weights,
            \item the encoder could be a deep network,
            \item $c(\bm{x})$ is viewed as a lossy compression of $\bm{x}$,
            \item can think of $c(\bm{x})$ as features of $\bm{x}$,
        \ei
        \item the associated cost function is
        \beq
            - \log p(\bm{x}|c(\bm{x})),
        \eeq
        \item as an example, if there is one linear hidden layer and the mean squared error criterion is used to train the network, then the $k$ hidden units learn to project the input in the span of the first $k$ principal components of the data,
        \item an autoencoder is usually used as component of neural network, such as feature extraction, and dimensionality reduction, where only the encoder is used after training,
        \bi
            \item autoencoders are not very effective in training deep networks.
        \ei
    \ei
    \item Autoencoder versus RBM:
    \bi
        \item taking $\bm{h} = c(\bm{x})$ in (\ref{rbm}, \ref{ae}), an autoencoder and RBM become equivalent when,
        \bi
            \item the encoder is one layer,
            \item the encoder and decoder are symmetric and share weights,
            \item an RBM uses the same activation as an autoencoder.
        \ei
    \ei
    \item Variation autoencoder:
    \bi
        \item using a Bayesian approach on an autoencoder, a \emph{variational autoencoder} (VAE) can generate new data,
        \bi
            \item generating new data from available one is called generative modeling,
        \ei
        \item in VAE, $\bm{z} \triangleq c(\bm{x})$ (see (\ref{ae})) is called the \emph{latent variables},
        \bi
            \item think of $\bm{z}_i$ as an object or a high level attribute of an object,
            \item $P(\bm{z})$ is assumed to be diagonal, unit Gaussian random vector,
        \ei
        \item assume $P(\bm{x}|\bm{z})$ \&  $P(\bm{z}|\bm{x})$  are also diagonal Gaussian, with mean and variances of $(\mu,\Sigma)$,
        \item the encoder \& decoder are used to predict the corresponding means \& variances,
        \beq\label{vae_1}\begin{CD}\begin{aligned}
            \bm{z} \in P(\bm{z}) & @>\text{VAE decoder}>> (\mu^x, \Sigma^x),\\
            \bm{x}  & @>\text{VAE encoder}>> (\mu^z, \Sigma^z),\\
        \end{aligned}\end{CD}\eeq
        \item during training the process is as follows
        \beq
            \bm{x} \rightarrow (\mu^z, \Sigma^z) \rightarrow \bm{z} \rightarrow (\mu^x, \Sigma^x) \rightarrow \bm{x}',
        \eeq
        \item the loss function is the \emph{reconstruction loss} which could be based on KL divergence rather than L2,
        \item to generate data,
        \beq
             \bm{z} \rightarrow (\mu^x, \Sigma^x) \rightarrow \bm{x}.
        \eeq
        \item Generative adversarial networks:
        \bi
            \item \emph{generative adversarial networks} is a framework for estimating generative models via an adversarial process, with two components
            \bi
                \item a generative model (G), similar to VAE decoder, generates fake data,
                \item a discriminator model (D), is a binary classifier that decides whether a data is real or generated by G.
            \ei
            \item generator and discriminator model are trained together,
        \ei
    \ei
\end{itemize}

\newpage
%---------------------------------------
\section{Applications\label{app_section}}
\begin{itemize}
    \item Andrew Ng: AI is the new electricity
    \bi
        \item rule of thumb: what humans can do in less than a second
    \ei
    \item Applications:
    \bi
        \item vision, Section \ref{vision_section}
        \item automatic speech recognition \& natural language processing, Section \ref{nlp_section}
        \item health care \& biomedicine, Section \ref{biomed_section}
        \item web searches, content filtering \& online advertising
        \bi
            \item what ad to provide to a given user?
        \ei
        \item personal assistant such as e-mail smart reply
        \item recommendations on e-commerce web sites
        \bi
             \item Netflix
        \ei
        \item automotive \& robotics
        \item analyzing particle accelerator data
        \item video games
        \item finance
        \item education
    \ei
    \item Search engine:
    \bi
        \item given a query, a search engines can also search for other phrases that have word vectors close to the query, see Section \ref{nlp_section},
        \item given a query and a document, a neural network can return a score for the query, document pair.
    \ei
    \item Some startups:
    \bi
        \item Capio, clarifai, calrify, Dato, emotient, enlitic, ersatz labs, EyeEm, herta security, IFLYTEK, Intelligent Voice, iQIY, Letv, megv11, MetaMind, NERVANA SYSTEMS, rbeus, SENSETIME, Sogou, Unisound, VIONVISION, zebra.
    \ei
\end{itemize}

\subsection{Health care \& Biomedicine\label{biomed_section}}
\begin{itemize}
    \item Medical image understanding
    \bi
        \item radiology
    \ei
    \item Massive amount of genomic data availability
    \item Bioinformatics:
    \bi
        \item can think of gene expression as object classification
        \item drug discovery
    \ei
    \item Molecular activity prediction
\end{itemize}

\newpage
\section{Visual Recognition\label{vision_section}}
\begin{itemize}
     \item Generalities:
    \bi
    \item vision recognition is also known as \emph{image perception}, \emph{machine vision}, \emph{scene understanding} or \emph{computer vision}
        \item computer vision enables computers to see, identify and process images in the same way that human vision does, and then provide appropriate output
        \bi
            \item the goal of computer vision is to extract useful information from an image
        \ei
        \item a camera looks at a 3D object and transforms them to 2D image
        \bi
            \item this transformation is modeled by a \emph{camera matrix} $C$
            \item computer vision can infer depth resolution, i.e. 3D
        \ei
        \item compared to other sensors
        \bi
            \item images are characterized as having high spatial resolution
            \item cameras and image processing are cheap
        \ei
    \ei
    \item A list of vision tasks:
    \bi
        \item classification (of a single object)
        \bi
            \item \emph{classification} implies single object classification
            \item classification partitions images to $C$ classes, or labels
            \item e.g., face recognition, identity verification
        \ei
        \item localization (put box around an object), see below
        \item classification + localization
        \bi
            \item generates both the label as well as the location of an object
        \ei
        \item detection (classification + localization of variable number of objects)
        \item semantic segmentation (classification at pixel level)
        \item instance segmentation
        \item attention models
        \item video \& action recognition
        \item 3D and depth interpretation
    \ei
    \item Localization:
    \bi
        \item \emph{localization} implies localizing a classified object
        \item given an image, generate box around object
        \bi
            \item box means object coordinates at upper left corner $(x,y)$, width and height $(w,h)$
            \item sometimes referred to as \emph{bounding-box regression}
        \ei
        \item can be approached as a regression problem, where the metric is the L2 distance between estimated box and ideal box coordinates
        \item FC layers do not preserve spatial information, while convolutional layers do
    \ei
    \item IoU:
    \bi
        \item intersection over union, or IoU, is a metric used to measure performance of localization and segmentation problems
        \item IoU is a number between zero and one, with higher IoU's implying better performance
        \item for localization, IoU is the intersection of the box and the object, divided by their union
        \item for segmentation, IoU is the intersection of the reference and classified pixels in a given class, divided by their union
    \ei
    \item Localization of $K$ objects:
    \bi
        \item $K$ is the number of objects in an image
        \item for example, $K=4$
        \beq
            \{ \text{cat,  cat's head,  left ear,  right ear} \}
        \eeq
        \item in the above example, the output is $4K$ values
    \ei
    \item Image location to other regression tasks:
    \bi
        \item the localization regression technique can also be used to characterize other real valued properties of an object
        \item for example, human pose estimation
    \ei
    \item Detection \& mAP:
    \bi
        \item \emph{detection} is the classification and the localization of variable number of objects,
        \item evaluation is through the metric \emph{mean average precision}, or mAP
        \bi
            \item mAP computes average precision separately for each class, then averages over classes,
            \item  $0 \leq \text{mAP} \leq 100$, high is good
        \ei
    \ei
\end{itemize}

\subsection{Color}
\begin{itemize}
    \item Color space:
    \bi
        \item a \emph{color space} is a specific organization of colors
        \item a color space provides a way to categorize colors and represent them in digital images
        \item the color space of a pixel is usually represented by a triplet while greyscale is a scalar
    \ei
    \item Hue:
    \bi
        \item hue represents color independent of any change in brightness
        \item \emph{hue} is in degrees on the color wheel
        \bi
            \item hue is a cylindrical-coordinate representation
        \ei
        \item let $V_\text{max} \triangleq \max(R,G,B)$, $V_\text{min} \triangleq \min(R,G,B)$ and $\Delta = V_\text{max} - V_\text{min}$, then the hue $H$ is
        \beq\begin{split}
            H = \frac{30}{\Delta} (G-B),& \qquad \text{if }V_\text{max}=R\\
            H = 60 + \frac{30}{\Delta} (B-R),& \qquad \text{if }V_\text{max}=G\\
            H = 120 + \frac{30}{\Delta} (R-G),& \qquad \text{if }V_\text{max}=B\\
        \end{split}\eeq
        \bi
            \item $0$ (or $360/2$) is red, $120/2$ is green, $240/2$ is blue
        \ei
    \ei
    \item Lightness or value:
    \bi
        \item \emph{lightness} measures the relative lightness or darkness of a color
        \item lightness is a percentage value
        \bi
            \item $0\%$ is dark (black)
            \item $100\%$ is light (white)
        \ei
        \item if $V_\text{max} \triangleq \max(R,G,B)$, and $V_\text{min} \triangleq \min(R,G,B)$, then the lightness $L$ is
        \beq
            L = \frac{V_\text{max}+ V_\text{min}}{2}
        \eeq
    \ei
    \item Saturation:
    \bi
        \item \emph{saturation} is a measurement of colorfulness
        \bi
            \item as colors get lighter and closer to white, they have a lower saturation value, whereas colors that are the most intense, like a bright primary color have a high saturation value
        \ei
        \item saturation is a percentage value
        \bi
            \item $100\%$ is the full colour
        \ei
        \item if $V_\text{max} \triangleq \max(R,G,B)$, and $V_\text{min} \triangleq \min(R,G,B)$, then the saturation $S$ is
         \beq\begin{split}
            S = \frac{V_\text{max}- V_\text{min}}{V_\text{max}+ V_\text{min}}& \qquad \text{if }L < 0.5\\
            S = \frac{V_\text{max}- V_\text{min}}{2-(V_\text{max}+ V_\text{min})}& \qquad \text{if } L \geq 0.5\\
        \end{split}\eeq
    \ei
    \item Color space representations:
    \bi
        \item RGB (Red, Green, Blue)
        \item BGR (Blue, Green, Red)
        \item HSV (Hue, Saturation, Value)
        \item HLS (Hue, Lightness, Saturation)
        \item LUV (Luminance, while $(x,y) \rightarrow (u,v)$)
        \item Lab (Lightness,  a and b for the color opponents green-red and blue-yellow)
    \ei
\end{itemize}

\subsection{Dataset}
\begin{itemize}
    \item Data sets:
    \bi
        \item MNIST, see below
        \item CIFAR, see below
        \item ImageNet
        \bi
            \item \emph{ImageNet Large Scale Visual Recognition Challenge}, or ILSVRC
            \item a database of labeled images
        \ei
        \item vehicle image database, GTI, \url{http://www.gti.ssr.upm.es/data/Vehicle_database.html}
        \item KITTI, \url{http://www.cvlibs.net/datasets/kitti/}
        \item PASCAL VOC
        \item MS-COCO
    \ei
    \item MNIST data set:
     \bi
        \item  MNIST data set is a database of the $10$ digits
        \item \url{http://yann.lecun.com/exdb/mnist/}
        \item NIST stands for the United States' \textit{National Institute of Standards and Technology}
        \item MNIST is a \emph{modified} subset of two data sets collected by NIST
        \item specifics:
        \bi
            \item the images are greyscale, $(28 \times 28)$ pixels
            \item training data contains $60,000$ images
            \bi
                \item these images are scanned handwriting samples from $250$ people, half of whom were US Census Bureau employees, and half of whom were high school students
            \ei
            \item test data contains $10,000$ images
            \bi
                \item the test data was taken from a different set of $250$ people than the original training data (a group split between Census Bureau employees and high school students)
            \ei
        \ei
     \ei
     \item CIFAR:
     \bi
        \item  \url{https://www.cs.toronto.edu/~kriz/cifar.html}
        \item the CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset
        \item CIFAR-10 dataset:
        \bi
            \item $60,000$ images
            \bi
                \item $50,000$ training images and $10,000$ test images
            \ei
            \item $32 \times 32$ colour images
            \item $10$ classes
            \bi
                \item with $6000$ images per class
            \ei
        \ei
        \item CIFAR-100 dataset:
        \bi
            \item like the CIFAR-10, except it has $100$ classes containing $600$ images each
            \item there are $500$ training images and $100$ testing images per class
        \ei
     \ei
    \item Formats:
    \bi
        \item LMDB
        \item HDF5
    \ei
\end{itemize}

\subsection{Image Processing}
\begin{itemize}
    \item Edge detection:
    \bi
        \item an edge detection algorithm extracts edges from an image
        \item an image is converted to a binary image where the ones correspond to the edges
        \item approaches to detect edges
        \bi
            \item gradient based, for example by using Sobel operator or Canny Edge detection, where first gradient is taken and then passed through a threshold
            \item threshold based, by looking into some of the color space attributes and then passing it through a threshold
        \ei
    \ei
    \item Image convolution:
    \bi
        \item an image $f(x,y)$ is convolved with a \emph{filter}, or \emph{mask} $h(x,y)$
        \beq
            g(x,y) = h(x,y) \ast f(x,y)
        \eeq
        \item $h(x,y)$ is $(n \times n)$, when $n$ is odd, e.g. $n=3$
        \item applications include blurring, sharpening, edge detection, noise reduction etc.
        \bi
            \item Gaussian smoothing
            \item Sobel operator
        \ei
    \ei
    \item Sobel operator:
    \bi
        \item the Sobel operators or filters are defined as
        \beq
            S_x \triangleq \left(\begin{array}{ccc}
                      -1 & 0 & 1 \\
                      -2 & 0 & 2 \\
                      -1 & 0 & 1 \\
                    \end{array}\right), \qquad
            S_y \triangleq \left(\begin{array}{ccc}
                      -1 & -2 & -1 \\
                      0 & 0 & 0 \\
                      1 & 2 & 1 \\
                    \end{array}\right)
        \eeq
        \item the Sobel operators take the derivative of the image in the x (horizontal) or y (vertical) direction
    \ei
    \item Canny Edge Detection:
    \bi
        \item developed by John F. Canny in 1986
        \item the goal is to identify the boundaries in an image
        \item the Sobel operator is at the heart of the Canny edge detection algorithm
        \item the following steps are usually included:
        \bi
            \item convert image to greyscale
            \item apply Gaussian smoothing function to suppress noise and spurious gradients
            \item compute the gradient
            \item have two thresholds, where all edges detected above the high threshold are retained while pixels between two threshold are retained if connected to strong edges
        \ei
    \ei
    \item The Hough Transform \& lines:
    \bi
    \item developed by  Paul Hough in 1962
    \item \emph{Hough transform} represents lines in parameter space, or in \emph{Hough space}
    \bi
        \item the line $y=mx+b$ (in \emph{image space}) transforms to the point $(m,b)$ in Hough space
        \beq
            (x,y) \rightarrow (m,b)
        \eeq
        \item a point $(x_o,y_o)$ transforms to the line $(m,y_o-mx_o)$ over variable $m$
        \bi
            \item a line is transformed to a point
            \item a point is transformed to line
        \ei
        \item a line that passes through two points in image space is the intersection of the two lines, associated with the two points, in Hough space
    \ei
    \item instead of detecting lines, from multiple points, in image space, can detect intersection of lines in Hough space
  \ei
  \item Hough Transform in polar coordinates:
  \bi
    \item polar coordinates $(\rho,\theta)$, are used to address vertical line representations with $m=\infty$,
    \item draw a segment $s$, from origin that intersects perpendicular to $y=mx+b$
    \bi
        \item $\rho$ is the perpendicular distance of $s$
        \item $\theta$ is the angle at origin from x-axis to $s$
    \ei
    \item since the product of the slopes of perpendicular lines is $-1$
    \beq\begin{split}
            -1 = m \cdot \tan \theta \Rightarrow m = -\frac{\cos \theta}{\sin \theta}
    \end{split}\eeq
    \item using simple geometry
    \beq
          \sin \theta = \frac{\rho}{b}  \Rightarrow b= \frac{\rho}{\sin \theta}
    \eeq
    \item substituting $m$ and $b$ into the line equation
    \beq\begin{split}
            y &= mx + b\\
            &=  -\frac{\cos \theta}{\sin \theta}x+\frac{\rho}{\sin \theta}\Rightarrow\\
            x \cos \theta + y \sin \theta &= \rho
    \end{split}\eeq
    \item a point in image space $(x_o,y_o)$ corresponds to the sinusoidal equation in Hough space
    \beq
              x_o \cos \theta + y_o \sin \theta = \rho
    \eeq
    \item detecting lines (from points) in image space becomes equivalent to detecting intersection of sinusoidal equations in Hough space
    \ei
    \item Distortion:
    \bi
        \item during 3D to 2D transformation of a camera capture, distortion changes the shape and size of 3D objects captured on 2D image
        \item types of distortion:
        \bi
            \item radial distortion
            \item tangential distortion
        \ei
        \item a regular pattern, like a chessboard, can be used to calibrate a camera
    \ei
    \item Radial distortion:
    \bi
        \item cameras use curved lenses to form an image, and light rays often bend a little too much or too little at the edges of these lenses
        \item this creates an effect that distorts the edges of images, where lines or objects appear more or less curved than they actually are
        \item this phenomenon is known as \emph{radial distortion}, and it’s the most common type of distortion
        \item let $(x,y)$ be a point on the distorted image
        \item let $r$ be the distance from \emph{distortion center} to $(x,y)$
        \item to correct the appearance of radially distorted points in an image, a correction formula can be applied that uses three distortion coefficients $k1$, $k2$, $k3$, and $r$
        \beq\begin{split}
            x_c &= x(1+ k_1r^2 + k_2r^4+k_3r^6)\\
            y_c &= y(1+ k_1r^2 + k_2r^4+k_3r^6)\\
        \end{split}\eeq
    \ei
    \item Tangential distortion:
    \bi
        \item tangential distortion occurs when a camera’s lens is not aligned perfectly parallel to the imaging plane, where the camera film or sensor is
        \item this makes an image look tilted so that some objects appear farther away or closer than they actually are
        \item given the two distortion coefficients $p_1$ \& $p2$, the correction formulae is
         \beq\begin{split}
            x_c &= x + 2p_1 x y + p_2(r^2 + 2x^2)\\
            y_c &= y +  2p_2xy  + p_1(r^2 + 2y^2)
        \end{split}\eeq
    \ei
    \item Perspective transform:
    \bi
        \item a perspective transform maps the points in a given image to different, desired, image points with a new perspective
        \item the bird’s-eye view transform that let’s us view a lane from above is useful for calculating the lane curvature
    \ei
\end{itemize}


\subsection{Vision with legacy machine learning\label{vision_nn_section}}
\begin{itemize}
    \item Features:
    \bi
        \item raw pixel intensity
        \bi
            \item it can provide both color \& shape characteristics about an object
            \item for example, template matching
        \ei
        \item histogram of pixel intensity
        \bi
            \item it provides color characteristics about an object
            \item histograms lose shape characteristics but make detection less sensitive to orientation
            \item for example, saturation histogram forms a feature vector
        \ei
        \item gradient of pixel intensity
        \bi
            \item it provides shape characteristic of an object
            \item it loses color characteristics \& becomes color invariant
            \item for example, the direction of gradient can be used to identify a shape, see HOG below
        \ei
        \item recommended to use multiple features
        \bi
            \item need to normalize features if all features are represented as a single vector
        \ei
        \item finally, the features are fed to a classifier to be trained on data
    \ei
    \item HOG:
    \bi
        \item \emph{histogram of oriented gradients} (HOG) is a detection algorithm from $2005$
        \item HOG assumes that an object can be described by the distribution (histogram) of intensity gradients (edge directions)
        \item an image is divided into small connected regions called \emph{cells}, and for the pixels within each cell, a histogram of gradient directions is compiled
        \bi
            \item orientation is quantized to $M$ bins
            \item each pixel in a cell is assigned to one of the $M$ orientations
            \item each pixel influences on that bin based on its magnitude
            \item the resulting histogram is the feature vector associated with each cell
        \ei
        \item hyperparameters include cell size, cell overlap, quantized bins $M$,
        \item afterwards a linear classifier is applied on these features
        \item there are similarities to CNNs
    \ei
    \item DPM:
    \bi
        \item \emph{deformable parts model} (DPM), from $2010$, is based on HOG but uses more more complex functions than linear classifier
        \item DPM was state of the art before CNNs
    \ei
    \item Sliding window:
    \bi
        \item so far feature extraction and classification was performed on a single window on a larger image
        \item to be able to do detection, run classification at multiple locations \& scales on a high resolution image
        \item at each location generate score
        \item using heuristics, combine classifier \& regressor predictions across all scales for final prediction
    \ei
    \item Overfeat:
    \bi
        \item Overfeat \cite{overfeat} is an example of a sliding window architecture
        \item Overfeat was the winner of ILSVRC $2013$ classification + localization challenge
    \ei
\end{itemize}

\subsection{Vision with neural networks}
\begin{itemize}
    \item Classification and localization with CNN:
    \bi
        \item the network is the union of some CNN for classification, with a second FC \emph{regression-head}, appended to the network, at the top
        \item the resulting CNN will have two heads, a \emph{classification-head} and a regression-head
        \item the regression head gets its input either from the final convolutional layer, or after last FC layer
        \item the regression head is trained separately
        \item localization can be class agnostic, with $4$ coordinate numbers, or class specific, with $4\,C$ numbers
    \ei
    \item Detection as classification:
    \bi
       \item detection can be reduced to classification / localization problem by using moving window techniques
       \item need to test many positions and scales
    \ei
    \item Region proposal and selective search:
    \bi
        \item \emph{region proposal} algorithm looks only at a small subset of possible positions
        \item it finds “blobby” image regions that are likely to contain objects
        \item it performs class-agnostic object detector on a region
        \item one method of doing region proposal is called \emph{selective search} which is a bottom-up segmentation approach , merging regions at multiple scales
    \ei
    \item R-CNN \cite{rcnn}:
    \bi
        \item \emph{region based CNN}, or R-CNN is a region based algorithm proposal, implemented on a CNN
        \item using selective search module (not NN), called \emph{region proposal}, get around $2000$ different sized regions of interest (RoI)
        \item crop and warp \emph{each} region to some fixed size
        \item run CNN on each region of interest
        \item the classification-head used was SVM with capability of classifying $C=21$ different objects
        \item R-CNN is slow, with multistage training pipeline
    \ei
    \item Fast R-CNN \cite{fastrcnn},
    \bi
        \item to minimize process time
        \beq
            \text{R-CNN} \rightarrow  \text{Fast R-CNN}
        \eeq
        \item in fast R-CNN, an image goes through CNN once, not per RoI
        \item the flow is as follows
        \beq\begin{CD}
            \bm{x} @>\text{CNN}>> \text{feature map} @>\text{region proposal}>> \text{RoIs}\\
            @. @. @V1 \text{layer}VV \\
            @.\text{classifier, loc.} @<\text{FCs}<< \text{pooled RoI}
        \end{CD}\eeq
        \item region proposal module is not NN
        \bi
            \item region proposal is the bottleneck in fast R-CNN
        \ei
        \item course localization in inferred from the RoI, fine localization from regression head
    \ei
    \item RPN:
    \bi
        \item RPN stands for \emph{region proposal network}
         \item RPN takes an image as input and outputs a set of rectangular object proposals, each with a score
         \item RPN is modeled as a fully deep convolutional network
         \item RPN is trained to produce region proposals directly; no need for external region proposals
         \item for each position $N$ rectangular sizes are considered called \emph{anchor boxes}
    \ei
    \item Faster R-CNN \cite{fasterrcnn}:
    \bi
        \item with \emph{faster R-CNN}, region proposal is done by RPN
        \item the entire system is a single, unified network for object detection
        \item two level of decisions:
        \bi
            \item whether there is a RoI
            \item what the object is
        \ei
        \item results were further improved by using faster R-CNN with ResNet \cite{he2015}
    \ei
    \item YOLO:
    \bi
        \item You only look once (YOLO) is a state-of-the-art, real-time object detection system
    \ei
    \item SSD:
    \bi
        \item Single Shot MultiBox Detector, or SSD, is a method for detecting objects in images using a single deep neural network
    \ei
    \item Image segmentation:
    \bi
        \item bounding boxes are not practical for some objects such as curvy roads, a forest or a sky
        \bi
            \item bounding boxes achieve partial scene understanding
        \ei
        \item image segmentation aims to group perceptually similar pixels into regions and is a fundamental problem in computer vision
        \item two segmentation types:
        \begin{enumerate}
            \item semantic segmentation
            \item instance segmentation
        \end{enumerate}
        \item image segmentation is part of scene understanding
    \ei
    \item Semantic segmentation:
    \bi
        \item with \emph{semantic segmentation}, every pixel in an image is classified to belong to some object
        \item different instances of the same object are not differentiated
        \item if different object types are labeled by different colors, then every pixel in an image is colored by a classification
    \ei
    \item Semantic segmentation using NN:
    \bi
        \item semantic segmentation can be performed by extracting patches, performing patch classification, and by labeling the patch center-pixel by that object
        \bi
            \item this brute-force method is straight forward but tedious and expensive
        \ei
        \item similar to detection (R-CNN), can run CNN on the whole image once, and then pick patches from higher layers
        \item \emph{refinement} combines CNN and recurrence by using raw image and output of previous iteration as inputs for next iteration
    \ei
    \item Upsampling:
    \bi
        \item \emph{upsampling} addresses the issue of mapping CNN upper layers (with lower dimensions) to higher (pixel) dimensions
        \bi
            \item upsampling is the opposite of downsampling that occurs in generic CNNs
        \ei
        \item upsampling can be generated using a union of inner layers using skipping
        \item another approach is to use convolution type technique, where a filter will take a single input and will copy it to all $(n \times n)$ output values, each scaled by some filter weight $w_{ij}$
        \bi
            \item a stride length of $>1$ is used, at the output rather than the input
            \item overlapping outputs, from multiple filters, sum their values
        \ei
        \item sometimes referred to as \emph{backward strided convolution}
    \ei
    \item FCN:
    \bi
        \item a network constructed exclusively from convolutional layers is called \emph{fully convolutional network}, or FCN
        \bi
            \item locality is preserved in such networks \& is used in semantic segmentation
        \ei
        \item characteristics of FCNs:
        \begin{enumerate}
            \item replace FC layers with $(1 \times 1 \times F)$
            \item upsampling through the use of transposed convolutional layers
            \item uses skip connections for better segmentation
        \end{enumerate}
        \item an FCN is comprised of two parts: encoder \& decoder, see Section \ref{rnn_section}
        \bi
            \item encoder:
            \bi
                \item the goal of encoder is to extract features from image
                \item transfer learning is usually used, where the encoder can be one of the available CNN networks such as VGG16, see Section \ref{tl_sect}
               \item the last layer of encoder is $(1 \times 1 \times F)$ layer
            \ei
            \item decoder:
            \bi
                \item the decoder upscales the encoder output back to the original size of image
                \item decoder uses transposed convolutional layers
            \ei
        \ei
    \ei
    \item FCN-8:
    \bi
        \item an implementation of FCN from Berkeley
        \item the encoder for FCN-8 is the VGG16 model pretrained on ImageNet for classification
        \item the fully-connected layers are replaced by 1-by-1 convolutions
    \ei
    \item Instance segmentation:
    \bi
        \item \emph{instance segmentation} is similar to semantic segmentation, however different instances of the same object are differentiated
        \item sometimes called \emph{simultaneous detection and segmentation} or SDS
        \item can use combination of
        \bi
            \item region proposal network (RPN)
            \item capturing RoI and passing them through CNN to generate segmentation masks
            \item mask our background and predict object class
        \ei
    \ei
    \item Attention:
    \bi
        \item visual attention is the ability to focus on a certain region of an image with high resolution, while perceiving the surrounding image in low resolution
        \item in NLP, see Section \ref{nlp_section}, attention is the ability to focus on certain words in a sentence
        \bi
            \item examples include translation, question \& answer
        \ei
    \ei
    \item Visual Attention with captioning, a case study \cite{xu2015}:
    \bi
        \item attention can be used in image captioning, where the network flow is as follows
        \beq\begin{CD}
                @. @. @. @<<<\\
                @. @. @. @VVV @AAA\\
            \bm{x} @>\text{CNN}>> a @>\phi>> \bm{z}_t  @>\text{LSTM}>> \bm{h}_{t-1} @>\text{NN}>> y_t\\
               @. @VVV @A\alpha_iAA @VVV\\
               @. @>>> \text{f} @<<<
        \end{CD}\eeq
        \bi
            \item $a$ is a set of $L$ features, with $\bm{a}_i \in \mathbb{R}^d$
            \item the \emph{context vector} $\bm{z}_t$ is a dynamic representation of the relevant part of the image input at time $t$
            \item $\bm{h}_t$ is the hidden state of an LSTM network
            \item the caption is
            \beq
                y = \{\bm{y}_1, \cdots, \bm{y}_C \}, \;\; \bm{y}_i \in \mathbb{R}^K
            \eeq
            where $K$ is vocabulary size and $C$ is caption length
            \item $f$ is the \emph{attention model}, where ignoring normalization
            \beq
                \alpha_{ti} = f(\{\bm{a}_i\},\bm{h}_{t-1})
            \eeq
            \item given the weights $\alpha_i$
            \beq
                \bm{z}_t = \phi (\{\bm{a}_i\},\{\alpha_i\}).
            \eeq
            \item \emph{soft attention} corresponds to interpreting $\{\alpha_{ti}\}$ as probabilities of locations
            \item \emph{hard attention} corresponds to picking a single location, or non-zero $\alpha_{ti}$
            \bi
                \item hard attention gradient descent not effective
                \item one proposal is to use reinforcement learning
            \ei
        \ei
    \ei
    \item Spatial transformer module \cite{jaderberg2016}:
    \bi
        \item \emph{the spatial transformer module}, included in a standard neural network architecture transforms deformed inputs to non-deformed ones by providing spatial transformation capabilities
        \item the action of the spatial transformer is conditioned on individual data samples, with the appropriate behaviour learnt during training
        \item the spatial transformer module is a dynamic mechanism that can actively spatially transform an image by producing an appropriate transformation for each sample
        \item  the transformation is then performed on the entire feature map (non-locally) and can include scaling, cropping, rotations, as well as non-rigid deformations
        \item the spatial transformer has three component:
        \bi
            \item the \emph{localisation network} predicts a transformation to apply to the input image input sample, this is where attention comes is
            \item the \emph{grid generator} generates a set of points where the input map should be sampled to produce the transformed output
            \item given the feature map and the sampling grid, the \emph{sampler} produces the output map, sampled from the input at the grid points
        \ei
        \item insert spatial transformer into a classification network
    \ei
    \item Dense trajectories in action recognition:
    \bi
        \item dense trajectories is a legacy approach to action recognition
        \item idea is to find key points \& track them using tracklets
        \bi
            \item a tracklet is $15$ set of $(x,y)$ coordinates
        \ei
        \item three steps to do this:
        \bi
            \item detect feature points
            \item track each key point feature using \emph{optical flow}
            \bi
                \item optical fields provide motion field
            \ei
            \item extract HOG/HOF/MBH time-based features in the coordinate system of each tracklet
            \bi
                \item HOG is generalized to include time
                \item HOF is \emph{histogram of flow}
            \ei
            \item the output of histograms was processed by an SVM
        \ei
    \ei
    \item NN \& action recognition:
    \bi
        \item one approach to include time, is to extend the convolutional filters, in a CNN, by adding a fourth dimension
        \bi
            \item these filters are referred to as 3D filters to emphasize their spatio-temporal nature,
            \item 3D filters are useful for local, short-term events
            \item the fusion of time into the network can be done at different levels: slow, early, late or single-frame fusion
        \ei
        \item another approach is to have two networks, one dedicated to images and one to temporal flow, and then fusing the two
        \item if global motion need to be captured (i.e. longer than a fraction of a second), then use RNN or LSTM
        \item can combine CNN (with or without 3D filters) with LSTMs
        \item another proposal is to have each neuron in the CNN to have a local feedback term as well \cite{ballas2016}
        \bi
            \item this is similar to using a first order IIR filter rather than a an FIR filer
        \ei
    \ei
\end{itemize}

\newpage
%---------------------- Natural Language Processing -------------------
\section{Natural language processing\label{nlp_section}}
\begin{itemize}
    \item  Natural language processing:
    \bi
        \item \emph{natural language processing}, or NLP, is the study of the computational treatment of natural language
        \bi
            \item natural means human
        \ei
        \item the goal of NLP is to be able to design algorithms to allow computers to "understand" natural language to perform certain tasks
    \ei
    \item Related fields:
    \bi
        \item linguistics
        \item theoretical computer science \& AI
        \item statistics
        \item psychology
    \ei
    \item Linguistic Definitions:
    \bi
        \item \emph{syntax} refers to grammatical structure,
        \item \emph{semantics} refers to the meaning of the vocabulary symbols arranged with that structure
        \item  \emph{morphology} is the study of word components, how they are formed, \& their relationship to other words in the same language
        \item \emph{phonetics} is the study of sound
        \item \emph{linguistics} is the scientific study of language and its structure, including the study of morphology, syntax, phonetics, and semantics
        \item \emph{lexicon} is the set of words used in a language
        \item a \emph{corpus} is a collection of written texts
        \item \emph{n-gram} is a contiguous sequence of $n$ items from a given sequence of text or speech
    \ei
    \item Phonemes \& triphones:
    \bi
        \item a \emph{phoneme} is a single "unit" of sound that has meaning in any language
        \bi
            \item there are $44$ phonemes in English
        \ei
        \item a \emph{triphone} is a sequence of three phonemes
        \bi
            \item there are $44^3 = 85,184$ triphones in English
        \ei
        \item because of interference between neighboring pho\-nemes, the $44$ phonemes are not sufficiently distinguishable
        \item triphones address this interference
    \ei
    \item Automatic speech recognition:
    \bi
        \item \emph{automatic speech recognition}, or ASR, addresses the problem of converting spoken speech to text
        \item ASR's output is a \emph{transcription} which could be NLP's input
        \item conventional ASR is based on GMM-HMM model, described shortly
    \ei
    \item Challenges in ASR:
    \bi
        \item natural / conversational speech
        \item low SNR
        \item speaker variability- age, gender, accents
    \ei
    \item Traditional acoustic model:
    \bi
        \item feature extraction, such as spectrogram
        \item speaker adaptation
        \item phoneme prediction, such as GMM
    \ei
    \item Spectrogram:
    \bi
        \item \emph{spectrogram} is a two dimensional, time-frequency representation of voice
        \item through signal processing, voice is converted to a spectrogram
        \item spectrograms are generated using short term Fourier transforms (SFT)
        \item a spectrogram has similarities to an image, but its interpretation is not just a classification problem
        \item through filtering, background noise and echoes could be removed from a spectrogram
    \ei
    \item GMM:
    \bi
        \item the \emph{Gaussian Mixture Model} (GMM), is a statistical acoustic model
        \beq\begin{CD}
            \text{spectrogram, or observation vector, } x  @>\text{GMM}>>  \text{feature } p(x|s)
        \end{CD}\eeq
        where $s$ is a triphone (class)
        \item $p(x|s)$ is assumed to be a mixture of $M$ Gaussian distributions
        \beq
            p(x|s) = \sum_{m =1}^M c_{sm} \, N(x; \mu_{sm}, C_{sm})
        \eeq
        where for each component $m$ \& class $s$, there is an associated weight $c_{sm}$, mean $\mu_{sm}$ and covariance $ C_{sm}$
    \ei
    \item GMM training:
    \bi
         \item the GMM parameters are trained iteratively using the expectation maximisation (EM) algorithm
         \item the expectation step estimates $p_m(x|s)$ for all $m$ \& the data points $x_i$ in class $s$
         \item the maximization step updates the parameters of the model using the following formulas:
         \beq\begin{split}
            \mu_{sm} &= \frac{\sum_{i=1}^N p_m(x_i|s)\, x_i}{\sum_{i=1}^N p_m(x_i|s)}\\
            C_{sm}   &= \frac{\sum_{i=1}^N p_m(x_i|s)\, (x_i- \mu_{sm})(x_i- \mu_{sm})^T}{\sum_{i=1}^N p_m(x_i|s)}\\
            c_{sm} &= \frac{1}{N} \sum_{i=1}^N p_m(x_i|s)
         \end{split}\eeq
         where $N$ is the number of training points that belong to the class $j$
    \ei
    \item HMM:
    \bi
        \item hidden Markov-model (HMM) was a breakthrough technology in speech recognition during the 1970's
        \item an HMM considers triphones as its states $s$, and reconstruct words \& sentences
        \item the trellis of HMM is constrained, i.e.  not fully connected
        \item using merged/shared states, the triphone states are reduced from
        \beq
             85,184 \rightarrow 10,000
        \eeq
        \item HMMs are the most complex part of an ASR system
        \item training the model involves estimating the transition probabilities, and the emission probability density functions
        \bi
            \item this is usually performed by an instance of the EM algorithm known as the Baum-Welch algorithm
            \item the expectation part of Baum-Welch algorithm is the BCJR algorithm
        \ei
    \ei
    \item HMM-DNN:
    \bi
        \item starting around $2011$, DNNs replaced GMMs as acoustic models
        \bi
            \item DNN replaced the whole acoustic model
        \ei
        \item a DNN generates posterior probabilities $p(s|x)$ rather than $p(x|s)$
        \item this hybrid system dominates current ASR
    \ei
    \item HMM-free RNN recognition:
    \bi
        \item an active research area is the replacement of HMMs with RNNs
        \bi
            \item RNNs seem to work well as a replacement for both DNN and HMM
        \ei
        \item phonemes can be replaced by characters as the building block
        \item the acoustic model outputs $p(a|s)$ where $a$ is a character
        \item blanks and junk are represented as underscore
        \item using dynamic programming, character sequences are converted to words, multiple characters are collapsed, etc.
    \ei
    \item Word vectors:
    \bi
        \item the English vocabulary $V$ is around $|V| = 1$ million words
        \item if each word forms a vector basis we end up with a one million dimensional space
        \item since words are related, or correlated, the space could be shrunk to $d \ll 10^{13}$ dimensions
        \bi
            \item $d = 100-500$ is a good range
            \item  each dimension would encode some meaning that we transfer using speech
            \item this is known as distributed representation of a word
        \ei
        \item thus, we will treat words as a real valued vector
        \beq
            w \in \mathbb{R}^d
        \eeq
        called \emph{word vectors}
        \item correlation between vectors is a measure to how related these words are
    \ei
    \item SVD based methods to generate word vectors:
    \bi
        \item see Appendix \ref{math_section}, for SVD review,
        \item word $w_j$ is \emph{near} word $w_i$ if its within some window $t$ away from  $w_i$,
        \item given a corpus, construct a $(|V|\times |V|)$ \emph{co-occurrence matrix} $X$, where $X_{ij}$ is the number of times word $w_j$ was found near $w_j$,
        \beq
            X_{ij} = X_{ji},
        \eeq
        \item perform \emph{singular value decomposition} (SVD) on $X$ and pick $d$ vectors based on largest $d$ eigenvectors,
        \item this is a global approach and is not practical.
    \ei
    \item Language models:
    \bi
        \item a \emph{language model} computes the joint probability associated with a sequence of words
        \beq
            p(w_1, \cdots, w_n),
        \eeq
        \bi
            \item consistent sentences are associated with high probabilities,
        \ei
        \item the \emph{unigram} model is very simple,,
        \beq\label{unigram_dist}
              p(w_1, \cdots, w_n) = \prod_{i=1}^{n} p(w_i),
        \eeq
        \item the \emph{bigram} model is a Markov chain with single memory,
         \beq
              p(w_1, \cdots, w_n) = \prod_{i=2}^{n} p(w_i|w_{i-1}).
        \eeq
        \item more generally, the conditional probability is conditioned over a causal or anti-causal window $m$,
        \beq\label{cp_window}
            p(w_t|w_{t-m},\cdots,w_{t-1},w_{t+1},\cdots,w_{t+m}).
        \eeq
    \ei
    \item Word vectors to conditional probabilities:
    \bi
        \item given word vectors $u$ and $v$, the conditional probability $p(u|v)$ is assumed to be of the form,
        \beq\label{cv2cp}
            p(u|v) = \frac{\exp (u^T v)}{\sum_{i=1}^{|V|} \exp (u_i^T v)},
        \eeq
        \item in other words, the softmax activation (\ref{softmax_def}) of the inner products of two word vectors, is the conditional probability.
    \ei
    \item Continuous Bag of Words Model:
    \bi
        \item both the Continuous Bag of Words Model and Skip-Gram model were proposed by \emph{Mikolov} et al.,
        \item in \emph{Continuous Bag of Words Model} (CBOW) model, the center word is predicted from the surrounding context,
        \item using ML criterion (\ref{ml_criterion}) and (\ref{cp_window}),  minimize
        \beq\label{ml_window}
            \min[- \log p(w_t|w_{t-m},\cdots,w_{t+m})],
        \eeq
        \item define
        \beq
            \hat{v} \triangleq \frac{w_{t-m}+\cdots w_{t-1}+w_{t+1}+\cdots w_{t+m}}{2m},
        \eeq
        \item as the name CBOW implies, the order of words in the window does not influence the projection $\hat{v}$,
        \item relabeling $u \triangleq w_t$, the ML criterion (\ref{ml_window}) is approximated to
        \beq\label{ml_window2}
            \min[ - \log p(u|\hat{v})],
        \eeq
        \item substituting in (\ref{cv2cp}), we want to minimize,
        \beq\label{cbow_model}\begin{split}
            &  -  \frac{\exp (u^T \hat{v})}{\sum_{i=1}^{|V|} \exp (u_i^T \hat{v})}\Rightarrow\\
            & -u^T \hat{v}  + \log \sum_{i=1}^{|V|} \exp (u_i^T \hat{v}),
        \end{split}\eeq
        \item this can be implemented by a single hidden layer that holds $\hat{v}$ and with an output layer with softmax activation (\ref{softmax_def}).
    \ei
    \item Skip-Gram model:
    \bi
        \item the \emph{Skip-Gram model}, the surrounding words are predicted from the center word,
        \item the objective function $J$, that we want to minimize becomes
        \beq\label{skip_gram_model}\begin{split}
         &  -\log p(w_{c-m},\cdots w_{c+m} | w_c)  \\
         \approx &  -\log \prod_{j=0,j \neq m}^{2m} p(w_{c-m+j}|w_c) \\
         = &  -\log \prod_{j=0,j \neq m}^{2m} p(u_{c-m+j}|v_c)\\
         = &  -\log \prod_{j=0,j \neq m}^{2m}  \frac{\exp (u^T_{c-m+j} v_c)}{\sum_{i=1}^{|V|} \exp (u_i^T v_c)}\\
         = & - \sum_{j=-m, \neq 0}^{m} u^T_{c+j} v_c+ 2m\log\sum_{i=1}^{|V|} \exp (u_i^T v_c).
        \end{split}\eeq
       \item given the objective function (\ref{skip_gram_model}), and a window, the gradient w.r.t each center vector $v$ and outside vectors $u$ can be computed,
        \item collectively denote the parameters that we can optimize as $\sigma \in \mathbb{R}^{2d|V|}$,
        \bi
            \item each word is associated with two vectors, one center and one outside,
        \ei
        \item then,
        \beq
            \theta^\text{new} = \theta^\text{old} - \eta \nabla_\theta J(\theta).
        \eeq
    \ei
    \item Negative sampling:
    \bi
        \item since $|V| \gg 1$ in (\ref{skip_gram_model}),  the resulting algorithm is not practical,
        \item instead of the softmax function, can train a binary logistic regression for the center word, and a randomly picked $k$ few word vectors to push down the probability,
        \item then a  cost function that is simpler than (\ref{skip_gram_model}), can be used,
        \beq
            -\log \theta(u^T_{c-m+j} v_c) - \sum_{j=1}^k E_{j \sim p(w)} \log \theta (-u^T_j v_c),
        \eeq
        \item note that $\theta(-x) = 1 - \theta(x)$,
        \item pick $p(w)$ to be the unigram distribution (\ref{unigram_dist}) to the power $3/4$ to amplify rare occurrences.
    \ei
    \item GloVe model:
    \bi
        \item the GloVe model is an alternate approach of NLP modelling where global and iterative features are combined,
        \item the cost function is
        \beq
            J(\sigma) = \frac{1}{2} \sum_{i,j=1}^{|V|} f(P_{ij}) (u_i^T v_j - \log P_{ij})^2,
        \eeq
        where $u_i^T v_j$ is a proxy for $P_{ij}$,
        \item can think of the $f$ function as a linear function that saturates for very high frequent words.
    \ei
    \item Neural network language model:
    \bi
        \item with neural networks, both the weights and code vectors are being optimized and are parameters,
        \item as a result, overfitting is a serious concern,
        \item various architectures for \emph{neural network language model} (NNLM),
        \item effective models include deep RNNs, and RNTN.
    \ei
    \item Recursive networks:
    \bi
        \item recurrent neural networks are based on a chain graph, while recursive networks form a parsing tree,
        \item unfortunately both systems are abbreviated by RNN,
        \bi
            \item when ambiguous, will refer to them as recurrent NN and recursive NN
        \ei
        \item a sentence is better modeled using a parsed tree rather than a chain,
        \bi
            \item recursion is helpful in describing natural language,
        \ei
        \item since chains are special case of a tree, recursive NN's are a superset of recurrent NN's.
    \ei
    \item Principle of compositionality:
    \bi
        \item phrases, like words, can be mapped to vectors,
        \item phrase vectors are generated using word vectors in that phrase, and combining them using some rule.
    \ei
    \item Standard recursive neural network:
    \bi
        \item given a parsed tree, a recursive neural network progresses from the leaves to the root of the tree,
        \bi
            \item recursive	NN's require a	parser	to get tree structure,
            \item when forming a parsing tree, the greedy algorithm can be used, using a score metric,
            \item the \emph{greedy algorithm} recursively makes locally optimal choices at each stage with the hope of finding a global optimum,
        \ei
        \item if $p_1$ and $p_2$ are two vectors associated with two phrases, then the concatenated phrase, represented by vector $q$, is modeled by a single layer neural network,
        \beq\label{composition}\begin{split}
            q &= \tanh ( W_1 p_1 + W_2 p_2  + b),\\
            s &= U^T p,
        \end{split}\eeq
        where $W_1, W_2 \in \mathbb{R}^{d \times d}$, $p_1, p_2, b, q \in \mathbb{R}^{d \times 1}$,
        \item the associated score is $s$,
        \item recursive implies all nodes of the tree would use the same $W_1, W_2$,
        \item the rule (\ref{composition}) can be expressed in a more compact form
        \beq\label{composition_b}
            q = \tanh (W p + b),
        \eeq
        where
        \beq\label{composition_c}\begin{split}
            W \triangleq (W_1, W_1),\qquad
            p \triangleq \left(\begin{array}{c}p_1 \\ p_2 \\\end{array}\right).
        \end{split}\eeq
    \ei
    \item Sentiment:
    \bi
        \item one application of recursive neural network is to classify a word, phrase, or sentence to a sentiment or a rating,
        \item the $n$-way sentiment $y^a$ associated with sentence $a$, can be extracted using the softmax function (\ref{softmax_def}),
        \beq
            y^a = \text{softmax}(W_s a),
        \eeq
        where $W_s \in \mathbb{R}^{n \times d}$, and $d$ is the dimensionality of $a$.
    \ei
    \item Determining tree structures using score:
    \bi
        \item the score $s(x,y)$ associated with tree $y$, and sentence $x$, is the sum of the parsing decision scores at each node (\ref{composition}),
        \beq
            s(x,y) = \sum_{n \in \text{nodes}(y)} s_n,
        \eeq
        \item the training set is $\{x_i, y_i\}$.
    \ei
    \item Recursive RNN variations:
    \bi
        \item Syntactically-Untied, or SU-RNN uses different weights $W$ depending on verb, noun etc.,
        \item Matrix-Vector, or MV-RNN associates a word with both a vector and a matrix,
        \bi
            \item the matrix is used to pre-multiply the vector of another phrase vector.
        \ei
    \ei
    \item Recurrent neural tensor networks:
    \bi
        \item \emph{recurrent neural tensor networks}, RNTN were used for sentimental classification,
        \item the goal is to generalize (\ref{composition_b}) by incorporating multiplicative interactions in the update rule to allow a greater interactions between the input vectors,
        \item given tensor $V \in \mathbb{R}^{2d \times 2d \times d}$, the generalized update rule (\ref{composition_b}, \ref{composition_c}) is
        \beq
           q = \tanh (p^T V p +   W p + b),
        \eeq
        \item when $V=0$ the standard recursive neural network is obtained.
    \ei
    \item CNN for NLP:
    \bi
        \item the idea is to compute all phrase combinations rather than using a parser,
        \item the convolution operator of a CNN is well suited for considering multiple parsing structures in parallel,
        \bi
            \item a convolutional layer computes all sequential $h$ word-vectors,
            \item a single convolution operation involves a filter $w \in \mathbb{R}^{hd}$, which is applied to a window of $h$ words to produce a scalar feature $c_i$,
            \item if CNN can handle $n$ words then a feature map $\bm{c} \in  \mathbb{R}^{n-h+1}$,
            \item multiple variations are possible,
        \ei
        \item a max pooling layer picks the most important feature by the function,
        \beq
            \hat{c} = \max \{\bm c \},
        \eeq
        \item will zero-pad shorter phrases,
        \item a CNN would use multiple (100s  of) different filters, with varying window sizes, resulting in multiple features,
        \item the pooled layer outputs are passed to a fully connected softmax layer whose output is the probability distribution over labels.
    \ei
    \item Machine translation:
    \bi
        \item consider
        \bi
            \item English language $e$ with model $p(e)$,
            \item French language $f$, with translation model $p(f|e)$,
        \ei
        \item using Bayes' rule, French to English translation can be interpreted as maximizing
        \beq
            \max_{e} p(e|f) = \max_{e} p(f|e) p(e),
        \eeq
        \item i.e., given $f$, using $p(f|e)$ generate multiple candidates for $e$'s, then a decoder computes the most probable joint probability.
    \ei
\end{itemize}


\newpage
%---------------------- Tools -------------------
\section{Tools\label{tools_section}}
\begin{itemize}
    \item GPU:
    \bi
        \item GPU is an effective hardware that accelerates parallel computations,
        \item a GPU is composed of multiple \emph{streaming multiprocessors} or SMs,
        \bi
            \item each SM includes multiple simple processors and a memory,
        \ei
        \item GPUs need to be explicitly parallel-programmed,
        \item compute capability is usually measured with \emph{floating point operations per second}, or FLOPS.
    \ei
    \item CPU:
    \bi
        \item Intel® Advanced Vector Extensions (AVX) is a set of instructions for doing Single Instruction Multiple Data (SIMD) operations on a CPU,
        \bi
            \item AVX2 allows for $32$ single precision FLOPS per second, per core
            \item power consumption is a concern and AVX2 is usually operated at lower frequencies,
            \item at $2500$ MHz, AVX2 generates $80$ GFLOPS,
        \ei
        \item FMA instruction set is an extension to SIMD instruction set to perform \emph{fused multiply add} (FMA) operations,
        \item AVX2 together with FMA could generate up to $500$ GFLOPS of computation from a single CPU.
    \ei
    \item GPU versus CPU:
    \bi
        \item GPUs optimize throughput whereas CPU optimizes latency,
        \item GPUs use more, but simpler processors than CPUs,
        \item FLOPS:
        \bi
            \item CPUs $\sim 500$ GFLOPS,
            \item GPUs $\sim 10$ TFLOPS.
        \ei
    \ei
    \item GPU interface:
    \bi
        \item fastest GPU interface is PCIe3 $\times 16$,
        \bi
            \item with PCIe3 each channel toggles at $8$ GT/s,
            \item over $16$ channels thats $16$ GB/s,
            \item compared to $10$ TFLOPs  $16$ GB/s may not be sufficient,
        \ei
        \item can connect multiple GPUs using  high-performance computer-networking communications standard such as \emph{InfiniBand}(IB).
    \ei
    \item NVIDIA Titan X:
    \bi
        \item GPU architecture is Pascal,
        \item $3584$ NVIDIA CUDA cores,
        \item Graphics Card Power is $250$ W,
        \item GPU engine base clock is $1417$ MHz,
        \item memory $12$ GB GDDR5X,
        \item $11$ TFLOPs.
    \ei
    \item System software i.e. drivers for GPU,
    \bi
        \item CUDA:
        \bi
            \item NVIDIA's CUDA is general purpose parallel computing tool that allows the user to write a single C code that runs on both CPU and GPU,
            \item \url{https://developer.nvidia.com/deep-learning/getting-started},
            \item higher-level APIs: cuBLAS, cuFFT, cuDNN, etc, see below.
        \ei
        \item OpenCL:
        \bi
            \item similar to CUDA, but runs on any GPU brand,
            \item usually slower than CUDA.
        \ei
    \ei
    \item CPU $\leftrightarrow$ GPU interactions in CUDA:
    \bi
        \item CPU is in charge,
        \item each CPU and GPU have their dedicated memory,
        \item CPU moves data back and forth the two memories using the command \emph{cudaMemcpy},
        \item CPU allocates memory on GPU memory using command \emph{cudaMalloc},
        \item GPU memory capacity may be limited,
        \item the host launches kernels on the device.
    \ei
    \item Parallel computing using CUDA
    \bi
        \item a \emph{thread} is one path of execution through the code,
        \item a CUDA command is of the form
        \bi
            \item name$<<<$blocks, threads$>>>$(d-out,d-in)
        \ei
        \item number of threads per block is $512$ on older GPUs, and $1024$ on newer GPUs,
        \item GPU is responsible of allocating blocks to SMs,
        \item the blocks and threads are in general three-dimensional entities denoted by
        \beq
            \text{dim}3(x,y,z),
        \eeq
        \item a \emph{map} operation with arguments elements and function, applies the function element-wise on each element.
    \ei
    \item Data versus model parallelism:
    \bi
        \item with \emph{data parallelism}, different data $\bm{x}_i$, are sent to different GPUs,
        \item with \emph{model parallelism}, the ANN is partitioned over multiple GPUs.
    \ei
    \item With distributed asynchronous gradient descent, different workers don't wait on other events to complete but continue processing data asynchronously.
    \item BLAS:
    \bi
        \item the BLAS (Basic Linear Algebra Subprograms) are low-level routines that provide standard building blocks for performing basic vector and matrix operations,
        \item they are the de facto standard low-level routines for linear algebra libraries,
        \item BLAS implementations take advantage of special floating point hardware,
        \item many numerical software applications use BLAS-compatible libraries to do linear algebra computations, including cuBLAS, Mathematica, MATLAB, NumPy, and R.
    \ei
    \item Software package trends:
    \bi
        \item many of the programming frameworks have performance libraries that harness the HW acceleration,
        \item NVIDIA's cuDNN, cuBLAS are optimized for DNN,
        \bi
            \item cuDNN provides a common set of tools that higher level frameworks can use/reuse,
            \item cuDNN identifies the $5\%$ of code that (such as convolutions, pooling, activation) takes $80\%$ of run time and delegates the execution of that code to GPU,
        \ei
    \ei
    \item List of software packages:
    \bi
        \item the four major packages are Caffe, Torch, Theano, Tensorflow,
        \item Kaldi, is specialized to speech recognition toolkit, \& is written in C++,
        \item NVIDIA's DIGITS, is an interactive system that provides a quick design capability and visual monitoring tools.
    \ei
    \item Caffe:
    \bi
        \item \url{http://caffe.berkeleyvision.org/},
        \item developed by U.C. Berkeley,
        \item written in C++ / CUDA,
        \bi
            \item do not need to write code to train,
            \item Python and Matlab interfaces optional,
            \item need to write C++ / CUDA for new GPU layers,

        \ei
        \item popular for CNN users,
        \item four main classes:
        \bi
            \item \emph{blobs} are tensors that store data, weights \& activations,
            \bi
                \item both data and gradients (diffs) are stored,
            \ei
            \item \emph{layers} interact with bottom blobs \& top blobs,
            \item a \emph{net} is a bunch of layers, or a graph,
            \item a \emph{solver} runs the net forward and backward,
        \ei
         \item makes use of \emph{protocol buffers}, (.proto) to define, for example, net \& solver
         \item many pre-trained models available through \emph{model zoo},
         \bi
            \item  good for fine-tuning existing networks.
         \ei
         \item not great for RNNs,
         \item cumbersome for big networks (GoogLeNet, ResNet).
    \ei
    \item Torch:
    \bi
        \item developed at NYU,
        \item written in C \& Lua,
        \bi
            \item Python and Matlab interfaces optional,
        \ei
        \item used \& maintained by Facebook \& Twitter,,
        \item uses tensors that are very similar to NumPy,
        \bi
            \item in addition to tensor, the nn module lets you easily build and train neural nets,
        \ei
        \item unlike NumPy, GPU is just a data-type away,
        \bi
            \item very easy to run code that runs on GPU,
        \ei
        \item uses \emph{modules} instead of net/layers (Caffe),
        \item many pre-trained models available,
        \item not great for RNNs.
    \ei
    \item Theano:
    \bi
        \item \url{http:/deeplearning.net/software/theano/},
        \item developed at University of Montreal,
        \item \emph{Theano} is a Python library for symbolic math, with compiler
        \bi
            \item is built on top on NumPy, but also shares similarities with SymPy
            \item its some combination of a programming language, a compiler and Python library,
            \item embraces computational graphs,
            \item automatically computes gradients, through symbolic differentiation,
        \ei
        \item Theano can run on either a CPU or a GPU/CPU,
        \bi
            \item C/C++ compiler on CPU
            \item CUDA/OpenCL compiler for GPU,
            \item using g++ compiler from TDM-GCC-64 for CPU, see \url{http://tdm-gcc.tdragon.net}.
        \ei
        \item a \emph{shared variable} lives in computational graph \&  persist call to call,
        \bi
            \item shared variables do not need to be identified as inputs in a function,
        \ei
        \item \emph{Keras} and \emph{Lasagne} are higher level wrappers around Theano, or Tensorflow
        \bi
            \item raw Theano is somewhat low-level,
            \item Lasagne sets up weights \& writes update rules for you,
            \item Keras is more high level than Lasagne,
        \ei
        \item works well for RNN,
        \item large models can have long compile times,
        \item “fatter” than Torch; more magic,
        \item patchy support for pre-trained models.
    \ei
    \item Tensorflow is discussed separately.
\end{itemize}


\newpage
%--------------------- VC Generalization -------------------
\section{VC Generalization Bound\label{vc_section}}
\begin{itemize}
    \item Hoeffding's inequality:
    \bi
        \item \emph{Hoeffding}'s inequality addresses the conditions under which generalization (\ref{generalization_def}) holds,
        \item Hoeffding's inequality is a form of large number theory that states
        \beq\label{hoeffdings_inequality}
            P[|C_\text{in}(h)-C_\text{out}(h)|>\epsilon] \leq 2 e^{-2\epsilon^2 N}
        \eeq
        \item the right hand side of (\ref{hoeffdings_inequality}) does not depend on $C_\text{out}(h)$,
        \item  Hoeffding's inequality can be applied during testing but not during learning.
    \ei
    \item Bounds with $M$ hypotheses:
    \bi
        \item when learning, all $M$ hypotheses may be considered,
        \item we do not want any of the $M$ hypothesis deviate from $E(h)_\text{out}$,
        \item then whatever hypothesis $g$ we chose we are OK,
        \item using the union bound, the probability that final hypothesis $g$ is bad (i.e. not tracking) is upper bounded as
         \beq\label{hoeffdings_inequality_b}
            P[|C_\text{in}(g)-C_\text{out}(g)|>\epsilon] \leq 2 M e^{-2\epsilon^2 N},
        \eeq
        \item it follows that as $M \rightarrow \infty$ the inequality gets looser.
    \ei
    \item Dichotomies \& growth functions:
    \bi
        \item $N$-tuple $(h(\bm{x}_1),\cdots h(\bm{x}_N))$ for classifier $h \in \mathcal{H}$, applied on a finite samples $\bm{x}_1,\cdots \bm{x}_N \in \mathcal{X}$, is called a \emph{dichotomy},
        \bi
            \item in other words, a dichotomy is an allowed hypothesis that assigns each vector to a one or a zero,
        \ei
        \item the \emph{growth function} $m_\mathcal{H}(N)$, is the maximum number of distinct dichotomies that can be generated with wisely chosen set of $N$ vectors,
        \beq
            m_\mathcal{H}(N)\triangleq \max_{\bm{x}_1,  \cdots \bm{x}_N \in \mathcal{X}} |\mathcal{H}|,
        \eeq
        \item the growth function is upper bounded by
        \beq
            m_\mathcal{H}(N) \leq 2^N,
        \eeq
        \item e.g., for the set of convex function hypotheses,
        \beq
            m_\mathcal{H}(N) = 2^N.
        \eeq
    \ei
    \item Break points $\Rightarrow$ polynomial growth functions:
    \bi
        \item the smallest value of $N$ for which $m_\mathcal{H}(N) \neq 2^N$ is called the \emph{break point},
        \item theorem: if the break point is less than infinity then  $m_\mathcal{H}(N)$ is polynomial in $N$,
        \item more specifically, \emph{Sauer's Lemma}  states that given a break point $k$,
        \beq\boxed{
            m_\mathcal{H}(N) \leq \sum_{i=0}^{k-1} \binom{N}{i}}
        \eeq
        \item note that there are no middle points; $m_\mathcal{H}(N) = 2^N$, or $m_\mathcal{H}(N)$ is polynomial in $N$ of order $(k-1)$.
    \ei
    \item Vapnik-Chervonenkis (VC) Inequality:
    \bi
        \item the \emph{Vapnik-Chervonenkis Inequality},
        \beq\label{hoeffdings_inequality_c}
            P[|C_\text{in}(g)-C_\text{out}(g)|>\epsilon] \leq 4 m_\mathcal{H}(2N) e^{-\tfrac{1}{8}\epsilon^2 N}
        \eeq
        is a tighter bound than (\ref{hoeffdings_inequality_b}),
        \item compared to (\ref{hoeffdings_inequality_b}), the growth function $m_\mathcal{H}(N)$ is substituted for $M$,
        \item VC inequality implies that as long as there is a break point, generalization from in sample to out of sample is possible.
    \ei
    \item VC dimension:
    \bi
        \item the \emph{VC dimension} of a hypothesis set $\mathcal{H}$, denoted by $d_{VC}(\mathcal{H})$, is equivalent to break point, where
        \beq
            d_{VC}(\mathcal{H}) \triangleq k-1,
        \eeq
        \item $d_{VC}(\mathcal{H})$ is finite $\Rightarrow  g \in \mathcal{H}$ will generalize,
        \bi
            \item independent of the learning algorithm,
            \item independent of the input distribution,
            \item independent of target function,
        \ei
        \item VC dimension is the effective binary degrees of freedom of a hypothesis,
        \bi
            \item it measures the effective number of parameters,
        \ei
        \item observation: larger VC dimensions need larger example set $N$ to learn,
        \item rule of thumb is to use $N \approx 10 \, d_\text{VC}$.
    \ei
    \item VC generalization bound:
    \bi
        \item denote the right hand side of (\ref{hoeffdings_inequality_c}) to be $\delta$,
        \beq\begin{split}
            \delta &\triangleq 4 m_\mathcal{H}(2N) e^{-\tfrac{1}{8}\epsilon^2 N}\Rightarrow\\
            \ln  \frac{\delta}{4 m_\mathcal{H}(2N)} &=  -\tfrac{1}{8}\epsilon^2 N\Rightarrow\\
            \epsilon  &=  \sqrt{\frac{8}{N}\ln \frac{4 m_\mathcal{H}(2N)}{\delta}}\\
            &\triangleq \Omega,
        \end{split}\eeq
        where $\Omega$ is called the \emph{generalization error},
        \item it follows that with probability $ \geq 1-\delta$,
        \beq\label{vc_omega}\begin{split}
            |C_\text{out}-C_\text{in}| &\leq \Omega(N,\mathcal{H},\delta)\Rightarrow\\
            C_\text{out}-C_\text{in} &\leq \Omega \Rightarrow\\
            C_\text{out} &\leq C_\text{in} + \Omega,
        \end{split}\eeq
        \item theorem: for any tolerance $\delta > 0$, with probability $\geq 1-\delta$,
        \beq\label{vc_gen_bound}\boxed{
             C_\text{out}(g) \leq C_\text{in}(g) +\sqrt{\frac{8}{N}\ln \frac{4 m_\mathcal{H}(2N)}{\delta}}}
        \eeq
        \item with a larger hypothesis set, we get a better approximation (smaller $C_\text{in}$)  but a worst generalization error (larger $\Omega$).
    \ei
    \item Classification versus estimation:
    \bi
        \item VC generalization bound can be generalized from classification problems to also include estimation problems,
        \item bias and variance analysis is an alternative to VC analysis, that is suited when squared error measure is used as an error metric.
    \ei
\end{itemize}

\newpage
\section{Bias \& Variance Method\label{bv_section}}
\begin{itemize}
    \item Average hypothesis:
    \bi
        \item let $g^{(\mathcal{D})}(\bm{x})$ be the final hypothesis associated with data set $\mathcal{D}$,
        \item the \emph{average hypothesis} $\bar{g}(\bm{x})$ is defined as
        \beq\label{avg_target}
            \bar{g}(\bm{x}) \triangleq E_\mathcal{D}[g^{(\mathcal{D})}(\bm{x})],
        \eeq
        \item in general $\bar{g}(\bm{x})$ is not known,
        \item $\bar{g}(\bm{x})$ is a function of the specific learning algorithm,
        \item given $N$, it is assumed that $\bar{g}(\bm{x})$ is the best hypothesis a family of hypothesis $\mathcal{H}$ can generate.
    \ei
    \item Bias or deterministic noise:
    \bi
        \item define \emph{bias} as
        \beq\label{bias_def}
            \text{bias} \triangleq E_x[(\bar{g}(\bm{x}) - f(\bm{x}))^2],
        \eeq
         \item the bias term is the part of the target function $f$ that $\mathcal{H}$ can not capture,
         \item an alternative name is \emph{deterministic noise},
         \item bias is related to approximation, i.e. on how well the hypothesis fits the data.
    \ei
    \item Bias \& variance method:
    \bi
        \item \emph{bias and variance} analysis is an alternate decomposition of $C_\text{out}$,
        \beq\label{e_out}
            C_\text{out}(g^\mathcal{D}) \triangleq E_x[(g(\bm{x}) - f(\bm{x})-n)^2]
        \eeq
        \item instead of $C_\text{in}$, the decomposition is w.r.t. the average hypothesis $\bar{g}(\bm{x})$,
        \bi
            \item as a result, the bias-variance analysis depends on the learning algorithm,
        \ei
        \item the \emph{variance} term,
        \beq\begin{split}\label{var_def}
            \text{var}  &\triangleq E_x[E_\mathcal{D}(g^{(\mathcal{D})}(\bm{x}) - \bar{g}(\bm{x}))^2],\\
        \end{split}\eeq
        is the variation in error due to different data sets and the corresponding hypotheses choices,
        \item let $\sigma^2_n$ denote the variance of stochastic noise,
        \item by introducing $\bar{g}(\bm{x})$ inside (\ref{e_out}), by taking expectations w.r.t. all data sets $\mathcal{D}$, and by substituting ( \ref{avg_target}, \ref{bias_def}, \ref{var_def}) into (\ref{e_out}), we obtain
        \beq\label{bv}
            E_\mathcal{D}[C_\text{out}(g^\mathcal{D})] = \text{var} + \text{bias} + \sigma^2_n,
        \eeq
        \item the bias \& variance analysis decomposes $C_\text{out}$ to three components:
        \begin{enumerate}
          \item the variance term quantifies the distance from a hypothesis $g^{(\mathcal{D})}$ to optimal hypothesis $\bar{g}$,
          \item the bias term quantifies the distance from optimal hypothesis $\bar{g}$ to target function $f$,
          \item the noise term quantifies the distance target function $f$ to observed quantities $y$,
        \end{enumerate}
        \item bias-variance analysis is a conceptual tool; in practice the terms can not be computed.
    \ei
\end{itemize}


\newpage
%---------------------- Support Vector Machines -------------------
\section{SVM's \& Kernel Methods\label{svm_sec}}
\begin{itemize}
    \item Support vector machine classifiers:
    \bi
        \item a \emph{support vector machine}, or SVM, is a learning algorithm for the perceptron
        \item SVM is a classifier based on supervised training
        \item two other classifiers with supervised learning are covered in Section \ref{math_section}:
        \bi
            \item naive Bayes
            \item decision tree
        \ei
    \ei
    \item Hard versus soft margin:
    \bi
        \item \emph{hard margin} SVM implies training data is linearly separable
        \item \emph{soft margin} SVM implies training data is not linearly separable
        \item will start with \emph{hard margin}, where in general, an infinite number of linear solutions classify data correctly
    \ei
    \item Distance:
    \bi
        \item relabel \& shorten $\bm{w}$ by pulling out the bias $b=w_0$, and dropping $x_0=1$
        \bi
            \item the resulting vector $\bm{w}$, which represents the decision hyperplane, is orthogonal to the hyperplane itself
        \ei
        \item define the \emph{distance} $ d(\bm{x}_n,\bm{x})$, between any $\bm{x}_\text{n} \in \mathbb{R}^d$ \& an arbitrary $\bm{x}$ on hyperplane, as the projection of $(\bm{x}_\text{n} - \bm{x})$ orthogonal to the decision hyperplane
        \beq\label{distance}\begin{split}
            d(\bm{x}_n,\bm{x}) &\triangleq
            \left|\frac{\bm{w}^T}{\|\bm{w}\|}\cdot (\bm{x}_\text{n} - \bm{x})\right| \\
             & = \frac{1}{\|\bm{w}\|} \left| \bm{w}^T\bm{x}_\text{n} +b - \bm{w}^T\bm{x}-b \right|\\
            & = \frac{| \bm{w}^T\bm{x}_\text{n} +b|}{\|\bm{w}\|}
        \end{split}\eeq
    \ei
    \item Margin:
    \bi
        \item an SVM determines the weight vector $\bm{w}$ in order to maximize the distance from the decision hyperplane to the nearest point(s)
        \item the minimum distance is called the \emph{margin}
        \item with SVM, the \emph{margin} around a hyperplane is maximized
            \item similar to minmax
    \ei
    \item Support vectors:
    \bi
        \item the nearest vectors (points) that define the margin are called the \emph{support vectors} (SV)
        \bi
            \item only SVs influence the weights, i.e. learning
            \item all other points are ignored
        \ei
    \ei
    \item SVM \& complexity $d_{VC}$:
    \bi
        \item imposing margin around hyperplane reduces the growth function
        \item as a result, $d_{VC}$ gets smaller which should help with generalization
    \ei
    \item Normalization:
    \bi
        \item normalize $(\bm{w}, b)$ such that for any $\bm{x}_\text{SV}$
        \beq\label{w_norm}
            |\bm{w}^T \bm{x}_\text{SV}+b| = 1
        \eeq
        \item since all $\bm{x}_\text{SV}$ are at same distance from hyperplane, the above normalization applies to all support vectors
        \item when $\bm{x}_n = \bm{x}_{SV}$, substituting (\ref{w_norm}) into (\ref{distance})
        \beq\label{distance_sv}\boxed{
             d(\bm{x}_{SV},\bm{x}) =\frac{1}{\|\bm{w}\|}}
        \eeq
        \item the constraint (\ref{w_norm}) can be re-interpreted as
        \beq\label{svm_constraint}\begin{split}
            \min_{\bm{x}_n \in \bm{x}_\text{SV}} |\bm{w}^T \bm{x}_\text{n}+b| &= 1 \Rightarrow\\
            \forall \bm{x}_n, \; y_n(\bm{w}^T \bm{x}_\text{n}+b) &\geq 1
        \end{split}\eeq
    \ei
    \item Margin violation with soft-margin:
    \bi
        \item with soft-margin, data points $(\bm{x}_n,y_n)$ are permitted to violate the margin by $\xi_n > 0$
        \item in other words, from (\ref{svm_constraint}), training data $(\bm{x}_n,y_n)$ violates the margin by  $\xi_n > 0$, if
        \beq\label{constraints_svm_c}
            y_n(\bm{w}^T \bm{x}_\text{n}+b) \geq 1 - \xi_n
        \eeq
        \item the \emph{total violation} is
        \beq\label{total_violation}
            \sum_i \xi_i
        \eeq
    \ei
    \item SVM optimization:
    \bi
        \item with hard margin SVM, \& under constraint (\ref{w_norm}), maximizing $1/\|\bm{w}\|$ (\ref{distance_sv}), is equivalent to minimizing
        \beq
              C = \frac{1}{2}\bm{w}^T\bm{w}
        \eeq
        \item with soft margin SVM, the total violation (\ref{total_violation}) is also included in the cost function
         \beq\label{svm_temp2}\begin{split}
             C = \min \frac{1}{2}\bm{w}^T\bm{w} +C \sum_i \xi_i
        \end{split}\eeq
        under the two sets of constraints, $\forall n$
        \beq\label{svm_temp2_constraints}\begin{split}
            y_n(\bm{w}^T \bm{x}_\text{n}+b) &\geq 1-\xi_n\\
            \xi_n &\geq 0
        \end{split}\eeq
        \item $C$ is usually determined through cross validation
        \bi
            \item $C$ trades error penalty for stability
            \item larger $C$ may minimize error rate on training data  but may also overfit
        \ei
        \item the associated Lagrangian $\mathcal{L}(\bm{w},b,\bm{\alpha},\bm{\xi})$, with the inequality constraints is known as KKT, and is given by
        \beq\label{lagrangian_svm}\begin{split}
              \mathcal{L} =\,\frac{1}{2}\bm{w}^T\bm{w} +C \sum_n \xi_n - \sum_n \beta_n \xi_n
               - \sum_n \alpha_n[y_n(\bm{w}^T \bm{x}_\text{n}+b)-1+\xi_n]
        \end{split}\eeq
        \item we want to minimize $\mathcal{L}(\bm{w},b,\bm{\alpha},\bm{\xi})$ w.r.t $(\bm{w},b)$ and $\bm{\xi}$, but maximize it w.r.t. $\alpha_n \geq 0$
    \ei
    \item $\mathcal{L}(\bm{w},b,\bm{\alpha},\bm{\xi})\rightarrow \mathcal{L}(\bm{\alpha})$:
    \bi
        \item differentiating w.r.t $(\bm{w},b)$ and $\bm{\xi}$
        \beq\label{derivatives_svm}\begin{split}
            \nabla_\omega \mathcal{L} &= \bm{w} - \sum_n \alpha_n y_n \bm{x}_n = 0\\
            \frac{\partial \mathcal{L}}{\partial b} &= \sum_n \alpha_n y_n = 0\\
            \frac{\partial \mathcal{L}}{\partial \xi_n} &= C -\alpha_n - \beta_n = 0
        \end{split}\eeq
        \item substituting (\ref{derivatives_svm}) into (\ref{lagrangian_svm}), SVM optimization is reduced to a maximization over $\bm{\alpha}$
        \beq\label{lagrangian_svm_alpha}
            \mathcal{L}(\bm{\alpha}) = \sum_{n=1}^{N}\alpha_n - \frac{1}{2}\sum_{n,m=1}^{N}y_ny_m\alpha_n\alpha_m \bm{x}^T_n \bm{x}^T_m
        \eeq
        under the constraints
        \beq\label{constraints_svm}\begin{split}
            &\forall n, \; 0  \leq \alpha_n \leq C\\
            &\sum_{n=1}^{N} \alpha_n y_n = 0
        \end{split}\eeq
    \ei
    \item Quadratic programming, $\mathcal{L}(\bm{\alpha})\rightarrow \bm{\alpha}$:
    \bi
        \item the optimization in (\ref{lagrangian_svm_alpha}) can be expressed as
        \beq\label{lagrangian_svm_alpha_b}\begin{split}
             \min_\alpha  \frac{1}{2}\bm{\alpha}^T Q \bm{\alpha} - \bm{1}^T \bm{\alpha}
        \end{split}\eeq
        where
        \beq\label{matrix_svm}
            Q_{ij} \triangleq  y_i y_j \bm{x}^T_i \bm{x}_j
        \eeq
        \item from (\ref{constraints_svm}), the constraints in vector form are
        \beq\label{constraints_svm_b}\begin{split}
            &\bm{0} \leq \bm{\alpha} \leq \bm{C}\\
            &\bm{y}^T \bm{\alpha}  = 0
        \end{split}\eeq
        \item there are tools available to solve such optimization problems, known as \emph{quadratic programming}
        \item since $Q$ is $(N \times N)$, such an optimization can become problematic for large $N$
    \ei
    \item Classes of $\bm{\alpha}$:
    \bi
        \item $\alpha_n = 0 \Rightarrow \bm{x}_n$ is an interior point,
        \item $0 < \alpha_n < C \Rightarrow \bm{x}_n$ is a \emph{margin support vector}, with $\xi_n = 0$
        \item $\alpha_n = C \Rightarrow \bm{x}_n$ is \emph{non-margin support vector}, with $\xi_n > 0$
    \ei
    \item $\bm{\alpha} \rightarrow \bm{w}, b$:
    \bi
        \item given $\bm{\alpha}$,  $\bm{w}$ is determined from (\ref{derivatives_svm})
        \item the KKT condition implies $\alpha_n = 0$ for all $n$ that are not support vectors,
        \item as a result $\bm{w}$ can be expressed as
         \beq\label{w_svm}
            \bm{w} = \sum_{\alpha_n > 0} \alpha_n y_n \bm{x}_n
        \eeq
        \item in PLA (\ref{pla}), all points that mismatch are updated whereas in (\ref{w_svm}) only the SV are used to derive $\bm{w}$
        \item the $b$ parameter can be solved from any SV, see (\ref{lagrangian_svm})
        \beq\label{svm_bias}\begin{split}
            y_i (\bm{w}^T \bm{x}_i +b) &= 1 \Rightarrow\\
            b &= \frac{1}{y_i} - \bm{w}^T \bm{x}_i\\
            &= \frac{1}{y_i} - \sum_{\alpha_n >0} \alpha_n y_n \bm{x}_n^T \bm{x}_i\\
        \end{split}\eeq
    \ei
    \item SVM in feature space \& the kernel:
    \bi
        \item consider a nonlinear space mapping
        \beq
            x \in \mathbb{R}^n \rightarrow   z \in \mathbb{R}^m
        \eeq
        \bi
            \item in general $m>n$
            \item the goal for this mapping is to make a classification problem linearly separable in the z-space, whereas it was not in the x-space
        \ei
        \item optimization in $z$-space proceeds similar to (\ref{lagrangian_svm_alpha_b}, \ref{constraints_svm_b}), but with the substitution  $\bm{x}_i \rightarrow \bm{z}_i$
        \item since sample size $N$ does not change, the only difference is the change in the inner products
        \beq\label{svm_tx}
            \bm{x}_i^T \bm{x}_j \rightarrow \bm{z}_i^T \bm{z}_j
        \eeq
        inside the Lagrangian (\ref{matrix_svm})
        \bi
            \item only the inner product of $\bm{z}$ is needed to do the computations, not $\bm{z}$ itself
            \item the dimensionality of $\bm{z}$ is not important
            \item this inner product $\bm{z}_i^T \bm{z}_j$ is some function of $(\bm{x}_i, \bm{x}_j)$, called the \emph{kernel}
            \beq\boxed{
                \bm{z}_i^T \bm{z}_j \triangleq K(\bm{x}_i, \bm{x}_j)}
            \eeq
        \ei
        \item when used with non-linear transformations, SVM's generate sophisticated boundaries (or hypothesis h) that are simply generated (i.e. simple $\mathcal{H}$)
    \ei
    \item Bound on $C_\text{out}$:
    \bi
        \item theorem: given the sample size $N$ and number of SV's, the out-of-sample performance can be bounded
        \beq
            E[ C_\text{out}] \leq  \frac{E[\# SV's]}{N-1}
        \eeq
        where $\# SV's$ is an in-sample determined value that is applied on out-of-sample estimate
        \item key takeaway is that this bound is independent of dimensionality $d$
        \bi
            \item we can transform space so that $\tilde{d} \rightarrow \infty$, without impacting $C_\text{out}$
        \ei
        \item this is the main theoretical result in support of SVM
    \ei
    \item $\bm{z}_i^T \bm{z}_j$ is sufficient statistics:
    \bi
        \item it was observed in (\ref{svm_tx}) that individual $\bm{z}_i$ vectors need not to be known to optimize the Lagrangian, just their inner product
        \item i.e., when using quadratic programming the $Q$ matrix components (\ref{matrix_svm}), become
        \beq\label{matrix_k}
            Q_{ij} \triangleq  y_i y_j K(\bm{x}_i, \bm{x}_j)
        \eeq
        \item similarly, only $\bm{z}_i^T \bm{z}_j$ is needed to compute $g$, since from (\ref{w_svm}, \ref{svm_bias}), $g(\bm{x})$ is given by
        \beq\label{svm_fn}\begin{split}
            & \text{sign}(\bm{w}^T \bm{z} +b) \\
            =& \text{sign}\left(\sum_{\bm{z}_n \text{is SV}} \alpha_n y_n (\bm{z}_n^T \bm{z} -\bm{z}_n^T \bm{z}_i) +\frac{1}{y_i}\right)\\
            =& \text{sign}\left(\sum_{\alpha_n > 0} \alpha_n y_n (K(\bm{x}_n,\bm{x}) -K(\bm{x}_n, \bm{x}_i)) +\frac{1}{y_i}\right)\\
        \end{split}\eeq
        \item note that the signal goes through two nonlinearities, $K$ and the sign()
        \item as long as we have access to $K(\bm{x}_i, \bm{x}_j) = \bm{z}_i^T \bm{z}_j$, we do not need to know about $z$-space
    \ei
    \item Kernel requirements:
    \bi
        \item two characteristics desired from a kernel:
        \bi
            \item the kernel should be valid, i.e., represent the inner product in some $\mathcal{Z}$ space
            \item the kernel should be a good match for the learning problem in hand
            \bi
                \item hand picking a kernel is a disadvantage compared to deep learning
            \ei
        \ei
        \item theorem: Mercer's condition, a kernel is valid iff for any $\bm{x}_1, \cdots, \bm{x}_N$
        \bi
            \item $K(\bm{x}_i, \bm{x}_j)$ is symmetric, and
            \item the $(N \times N)$ matrix with coefficients $K(\bm{x}_i, \bm{x}_j)$ is positive semi-definite
        \ei
    \ei
    \item Kernel examples:
    \bi
        \item kernels of space $\mathcal{Z}$, that supports polynomial order $Q$, can be computed efficiently in the form
        \beq
            K(\bm{x}_i,\bm{x}_j)= (b+a\bm{x}_i^T\bm{x}_j)^Q
        \eeq
        where $a$ and $b$ are scale factors
        \item the \emph{radial basis function} (RBS) kernel
        \beq\label{rbf_kernel}
            K(\bm{x}_i,\bm{x}_j) \triangleq e^{-\gamma \|\bm{x}_i-\bm{x}_j\|^2}
        \eeq
        is associated with an infinite dimensional $\mathcal{Z}$ space
    \ei
\end{itemize}

\newpage
%---------------------- Similarity Methods -------------------
\section{Similarity Methods}
\begin{itemize}
    \item Nearest neighbor method:
    \bi
        \item decode any $\bm{x}$ to its nearest data set of $N$ elements $(\bm{x}_i, y_i)$
        \item in other words, the data output is the same output as  its \emph{nearest neighbor} (NN)
        \item like ML decoding
        \item complexity is proportional to $N$
    \ei
    \item $k$-nearest neighbor method:
    \bi
        \item find the best $k$ closest neighbors, and make a decision based on the $k$ candidates $y_i$
        \item higher values of $k$ have a smoothing effect that makes the classifier more resistant to outliers
    \ei
    \item Bounded distance method:
    \bi
        \item this is similar to bounded distance decoding
        \item can think of a cylinder around each training data point
    \ei
    \item Radial basis function method:
    \bi
        \item after observing $N$ data samples, the \emph{radius basis function} hypothesis, or RBF, is
        \beq\label{rbf}
            h(\bm{x}) = \sum_{n=1}^{N} w_n e^{-\gamma \|\bm{x}-\bm{x}_n\|^2}
        \eeq
        \item the exponents form the basis functions
        \item since we have $N$ equations and $N$ unknowns, the exact interpolation solution ends up being
        \beq
            \bm{w} = \Phi^{-1} \bm{x}
        \eeq
        where $\Phi_{ij} = e^{-\gamma \|\bm{x}_i-\bm{x}_j\|^2}$
    \ei
    \item Bounded distance as an RBF:
    \bi
        \item when the basis function is chosen to be the cylinder function rather than Gaussian, RBF becomes bounded distance method
        \item can think of RBF to be the softened version of bounded distance method
    \ei
    \item Similarity methods:
    \bi
        \item the nearest neighbor, bounded distance and the RBF are examples of \emph{similarity methods} since we are comparing how similar data is to the training set
    \ei
    \item RBF with $K$ centers:
    \bi
        \item use $K \ll N$ centers instead of $N$, for better generalization,
        \item then the model of (\ref{rbf}) changes to
         \beq\label{rbf_k}
            h(\bm{x}) = \sum_{n=1}^{K} w_n e^{-\gamma \|\bm{x}-\bm{\mu}_n\|^2},
        \eeq
        \item the $K$ centers $\bm{\mu}_n$ are new parameters, each being a $d$ dimensional vector.
    \ei
    \item Weights for $K$-centers:
    \bi
        \item from (\ref{rbf_k}),
        \beq\label{k_center}
            \Phi \bm{w} = \bm{y},
        \eeq
        where $\Phi_{ij} = e^{-\gamma \|\bm{x}_i-\bm{\mu}_j\|^2}$,
        \item given the centers $\bm{\mu}_k$, or $\Phi$, there are $N$ equations and $K$ unknowns in (\ref{k_center}),
        \item such a system, can be solved similar to linear regression,
        \item if $\Phi^T\Phi$ is invertible,
        \beq
            \bm{w} = (\Phi^T\Phi)^{-1}\Phi^T \bm{y}.
        \eeq
    \ei
    \item Determining $\gamma$ through EM:
    \bi
        \item $\gamma$ in (\ref{rbf_k})can be determined iteratively, using \emph{expectation-maximization} (EM) algorithm,
        \bi
            \item given $\gamma$, solve for $\bm{w}$,
            \item given $\bm{w}$, minimize error w.r.t. $\gamma$.
        \ei
    \ei
    \item RBF versus SVM:
    \bi
        \item when the kernel is chosen as in (\ref{rbf_kernel}), the SVM solution is in the form, see (\ref{svm_fn})
        \beq\label{svm_exp}
            \text{sign}\left(\sum_{\alpha_n > 0} \alpha_n y_n e^{-\gamma \|\bm{x}_i-\bm{x}_j\|^2} + b\right),
        \eeq
        \item similarity with RBF can be noted by comparing (\ref{svm_exp}) to (\ref{rbf_k}).
    \ei
    \item Smoothness quantified:
    \bi
        \item smooth function are functions with small derivatives,
        \item smoothness can be measured by the metric
        \beq\label{smooth_def}
            \sum_{k=0}^{\infty} a_k \int_{-\infty}^{\infty} \left(  \frac{d^k h}{dx^k} \right)   dx.
        \eeq
    \ei
    \item RBF and regularization:
    \bi
        \item consider minimizing a function under smoothness (\ref{smooth_def}) constraint,
        \beq
            \sum_{n=1}^{N} (h(x_n)-y_n)^2 + \lambda \sum_{k=0}^{\infty} a_k \int_{-\infty}^{\infty} \left(  \frac{d^k h}{dx^k} \right)   dx
        \eeq
        \item when this is solved, we get RBF with Gaussian functions.
    \ei
\end{itemize}

\newpage
\section{Miscellaneous Mathematical Entities\label{math_section}}
\begin{itemize}
    \item Singular value decomposition:
    \bi
        \item $\forall \; (m \times n) \; \Omega, \; \exists$
        \bi
            \item unitary $(m \times m) \;U$
            \item unitary $(n \times n)\;V$ and
            \item non-negative real diagonal $(m \times n) \; D$
        \ei
        such that
        \beq\label{svd}
            \Omega = U D V^H
        \eeq
        \item this factorization is called \emph{singular value decomposition} (SVD)
    \ei
    \item SVD singular vectors \& singular values:
    \bi
        \item the columns of $U$, $\bm{u}_k$, are the eigenvectors of $AA^T$
        \bi
            \item $\bm{u}_k$ are called the \emph{left singular vectors}
        \ei
        \item the columns of $V$,  are the eigenvectors of $A^TA$
         \bi
            \item the columns of $V$, $\bm{v}_k$, are called the \emph{right singular vectors}
        \ei
        \item $D_{ii} >0$ are called the \emph{singular values} of $\Omega$
        \bi
            \item by convention, order diagonal entries such that

            \beq
                D_{i,i} \ge D_{i-1,i-1}
            \eeq
        \ei
        \item if there are $r \leq \min(m,n)$ singular values, then the rank of $\Omega$ in (\ref{svd}) is $r$
        \item $D_{ii}$ are the square roots of the nonzero eigenvalues of both $AA^T$ \& $A^TA$
        \item if all singular values of $\Omega$ are unique and non-zero, then its SVD is unique
        \bi
            \item this uniqueness is up to the multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor
        \ei
    \ei
    \item SVD \& vector compression:
    \bi
        \item let $D^{(s)}$ be the matrix derived from $D$, that keeps only the $s < r$ largest singular values, replacing the rest with zeros
        \item from (\ref{svd}), define rank  $s < r$ matrix
        \beq\begin{split}
            \Omega^{(s)} &\triangleq U D^{(s)} V^H\\
             &= \sum_{k=0}^{s-1}   \bm{u}_k  D_{kk} \bm{v}_k^T
        \end{split}\eeq
        \item each term $ \bm{u}_k  D_{kk} \bm{v}_k^T$ is called\emph{ principle image}
        \item $\Omega^{(s)}$ is the closest rank-l matrix to $\Omega$, where the term closest is in term of componentwise Euclidean norm
        \beq
            \sum_{i,j} (\Omega_{ij} - \Omega_{ij}^{(s)})^2
        \eeq
        \item the property of SVD to provide the closest rank-s approximation for a matrix $\Omega$ can be used to reduce vector dimensionality
        \bi
            \item in other words, SVD can be used to compress $\Omega$
        \ei
        \item such a compression is not necessarily the best way to compress images
    \ei
    \item Principle component analysis:
    \bi
        \item  \emph{principle component analysis}, or PCA, reduces the data set dimensionality from $d$ to $\tilde{d} < d$
        \item given $(N \times d)$ data matrix $X$, first the $(d \times d)$ covariance matrix $\overline{C}$, is computed (\ref{data_cov})
        \item $\overline{C}$ is symmetric and has a spectral factorization
        \beq\label{pca}
                \overline{C} = V \Lambda V^T
        \eeq
         \item the $d$ dimensional columns of $V$ are the eigenvectors of $\overline{C}$ called \emph{principle axes}
         \item the $(d \times d)$ diagonal $\Lambda$ contains the associated eigenvectors
         \item choose $\tilde{d}$ eigenvalues corresponding to the largest $\tilde{d}$ eigenvalues
         \item denote the resulting $(\tilde{d} \times d)$ matrix by $\tilde{V}$
         \item the projections of the data on the principal axes are called \emph{principal components}
         \item any data vector $\bm{x}$ can be compressed to $\tilde{\bm{x}}$ by
         \beq\label{pca_b}
            \tilde{\bm{x}} = \tilde{V} \bm{x}
         \eeq
    \ei
    \item Relating SVD to PCA:
    \bi
        \item when $X=\Omega$ (\ref{svd}),
        \beq\label{svd_x}\begin{split}
            X^TX &= (U D V^T)^T (U D V^T)\\
            &= V D U^T U D V^T\\
            &= V D^2 V^T
        \end{split}\eeq
        \item comparing (\ref{svd_x}) to (\ref{pca}),
        \beq
            D^2 = \Lambda.
        \eeq
    \ei
    \item Relative entropy:
    \bi
        \item the \emph{relative entropy} of $p(x)$ with respect to the \emph{entropy measure} $q(x)$ is defined as
        \beq\label{rel_entropy_def}
            D(p||q) \triangleq \sum_{x\in \mathfrak{X}} p(x) \log \frac{p(x)}{q(x)} =  E_p \log \frac{p(X)}{q(X)},
        \eeq
        \item $q(x)$ does not need to be a probability measure,
        \item when $q(x) = 1$, the entropy measure is known as the \emph{uniform measure},
        \bi
            \item with uniform measure, relative entropy  reduces to \emph{entropy},
            \beq
                H(X) \triangleq D(X||1),
            \eeq
            \item in that sense relative entropy generalizes entropy,
        \ei
        \item when $q(x)$ is a probability measure, then relative entropy is also called \emph{Kullback Leibler distance} between $p(x)$ and $q(x)$,
        \bi
            \item $D(p||q)$ is a measure of the inefficiency of assuming the distribution is $q$ when when the true distribution is $p$,
            \item $D(p||q) \geq 0$,
            \item $D(p||q) = 0 \Leftrightarrow p=q$,
            \item $D(p||q)$ is not a true distance since it is not \emph{symmetric} and does not satisfy the \emph{triangle inequality},
            \item $D(p||q)$ is \emph{convex} in the pair $(p,q)$.
        \ei
    \ei
      \item Bayesian criterion:
    \bi
        \item consider the hypotheses set $h \in \mathcal{H}$, \& a data set $\mathcal{D}$,
        \bi
            \item practical hypotheses sets are infinite,
        \ei
        \item in the \emph{Bayesian approach}, the posterior $P(h|\mathcal{D})$ is computed from prior $P(h)$, and the maximum $P(h=g|\mathcal{D})$ is chosen,
        \item it assumes the prior distribution on hypothesis set $P(h)$, is known,
        \bi
            \item this approach is justified when prior is valid or irrelevant.
        \ei
    \ei
    \item Maximum-likelihood criterion:
    \bi
        \item for independently generated data $\{\bm{x}_n\}$, the maximum-likelihood (ML) criterion maximizes the expression
        \beq\label{ml_criterion}\begin{split}
            \max_{\{\bm{y}_n\}} \;&\prod_{n=1}^{N} P(\bm{y}_n|\bm{x}_n) \Rightarrow\\
            \min_{\{\bm{y}_n\}} \;&-\log \left(\prod_{n=1}^{N} P(\bm{y}_n|\bm{x}_n)\right) \Rightarrow\\
            \min_{\{\bm{y}_n\}} \;& \frac{1}{N}\sum_{n=1}^{N}\log \frac{1}{P(\bm{y}_n|\bm{x}_n)}
        \end{split}\eeq
    \ei
    \item Naive Bayes model:
    \bi
        \item \emph{Naive Bayes model} is a subclass of Bayesian Network, BN, that makes "naive assumptions"
        \item often used in classification problems, where given feature observations $X_1, X_2, \cdots, X_n$ per sample, the model \emph{inferences} the class $C$
        \item analytically:
        \beq\begin{split}
            & \forall \, i,j\neq i, \; (X_i \perp X_j \;|\; C) \Rightarrow\\
            &P(C,X_1,  \cdots, X_n) = P(C) \, \prod_{i=1}^n P(X_i|C)\Rightarrow\\
            &P(c_i | x_1,\cdots,x_n) \propto P(c_i) \prod_{j=1}^n P(x_j|c_i) \Rightarrow\\
            & \hat{c} = \arg\max_{c_i} P(c_i) \prod_{j=1}^n P(x_j|c_i)
        \end{split}\eeq
        \item $P(c_i)$ is determined using the relative frequency of $c_i$ from the training data
        \item \emph{Gaussian Naive Bayes} model assumes $ P(x_j|c_i)$ is Gaussian
        \bi
            \item use MAP estimation to find $(\mu_i,\sigma_i)$
        \ei
    \ei
    \item Decision tree:
    \bi
        \item the space is partitioned into sub-regions by sequentially \& linearly splitting the space based on single features
        \item the resulting structure is a decision tree
        \item a decision is designed to maximize the \emph{information gain}, where
        \beq
            \text{information gain} = \text{parent-entropy} - \text{(weighted average) } \text{children-entropy}
        \eeq
    \ei
    \item Cross-entropy:
    \bi
        \item define \emph{cross-entropy} between two probability distributions $p$ \& $q$ over the same underlying set of events $X$ to be
        \beq\label{cross_entropy_def}
            H(p,q) \triangleq  -E_p \log q(X),
        \eeq
        \item expanding (\ref{rel_entropy_def}),
        \beq\begin{split}
             D(p||q) &=  E_p \log \frac{p(X)}{q(X)}\\
             &= E_p \log p(X) - E_p \log q(X)\Rightarrow\\
            H(p,q)  &= H(p) + D(p||q),
        \end{split}\eeq
        \item the cross entropy between two probability distributions measures the average number of bits needed to identify an event from a set of possibilities.
    \ei
     \item ML criterion \& cross-entropy:
    \bi
        \item consider the ML criterion (\ref{ml_criterion})  with scalar $x_n$, with $y_n \in \{0, 1\}$ and in the limit $N \rightarrow \infty$
        \beq\label{ml2ce}\begin{split}
        &\min_{\{y_n\}} \lim_{N \rightarrow \infty} \frac{1}{N}\sum_{n=1}^{N}\log \frac{1}{P(y_n|x_n)} \\
              =&\min_{\{y_n\}}  \lim_{N \rightarrow \infty} \frac{1}{N}\left(\sum_{n:y_n=1}\log \frac{1}{P(y_n = 1|x_n)} + \sum_{n:y_n=0}\log \frac{1}{P(y_n = 0|x_n)}\right)
        \end{split}\eeq
        \item define two terms
        \beq\label{p}\begin{split}
            q(x_n) \triangleq & \; P(y_n = 1|x_n)\\
            p \triangleq & \lim_{N \rightarrow \infty} \frac{\sum_{n=1}^N y_n}{N}
        \end{split}\eeq
        \item substitute $q(x_n)$ into (\ref{ml2ce})
        \beq\begin{split}
              =& \min_{p} \lim_{N \rightarrow \infty} \left(\frac{1}{N}\sum_{i=1}^{pN}\log \frac{1}{q(x_i)} + \frac{1}{N}\sum_{i=1}^{(1-p)N}\log \frac{1}{1-q(x_i)}\right)\\
              =& \min_{p} \left(p \, E_X \log \frac{1}{q(X)} +  (1-p)E_X \log\frac{1}{1-q(X)}\right)\\
        \end{split}\eeq
        \item compare this to minimizing the cross-entropy (\ref{cross_entropy_def})
        \beq\begin{split}\label{cross_entropy_der}
              = &  \min_{p} \;  E_p \frac{1}{\log q(X)}
        \end{split}\eeq
    \ei
    \item Linear regression:
    \bi
        \item linear regression has a long history in statistics
        \item given dataset $\{(\bm{x}_n,y_n)\}$, with $y_n \in \mathbb{R}$, find best linear fit that minimizes mean-square error
        \item from (\ref{in_sample}), the in-sample error can be represented as
        \beq\label{lin_reg}\begin{split}
            C_\text{in}(\bm{w}) &= \frac{1}{N} \|X\bm{w}-\bm{y}\|^2\\
            &=  \frac{1}{N}(X\bm{w}-\bm{y})^T (X\bm{w}-\bm{y})\\
            &= \frac{1}{N}(\bm{w}^TX^TX\bm{w} -2\bm{w}^TX^T\bm{y} +\bm{y}^T \bm{y})\\
        \end{split}\eeq
        where
        \bi
            \item $X$ is the $N \times (d+1)$ data matrix whose rows are the inputs $\bm{x}_n$
            \item  $\bm{y}= \{y_n\}$ is $(N \times 1)$
        \ei
        \item imposing $\nabla C_\text{in}(\bm{w})=0$ on (\ref{lin_reg})
        \beq\label{lin_reg_b}\begin{split}
             \nabla (\bm{w}^TX^TX\bm{w}) &= \nabla(2\bm{w}^TX^T\bm{y})\Rightarrow\\
           X^TX \bm{w} &=  X^T\bm{y}
        \end{split}\eeq
        \item if $(d+1) \times (d+1)$ $X^TX$ is invertible, a one-shot learning formulation is obtained
        \beq\label{pseudo_inv}\begin{split}
            \bm{w} &= (X^TX)^{-1}X^T\bm{y}\\
            &= X^\dag \bm{y}
        \end{split}\eeq
        where
        \beq
            X^\dag \triangleq (X^TX)^{-1}X^T
        \eeq
        is the pseudo-inverse of $X$
    \ei
    \item Gabor filter:
    \bi
        \item in image processing, a \emph{Gabor filter}, named after Dennis Gabor, is a linear filter used for edge detection,
        \item in the spatial domain, a 2D Gabor filter is a Gaussian kernel function modulated by a sinusoidal plane wave.
    \ei
      \item $K$-means clustering:
    \bi
        \item $K$-means clustering is a method of clustering data into $K$ classes where each cluster is identified by its center  $\bm{\mu}_k$,
        \item each center $\bm{\mu}_k$ is the representative of a group of data, called \emph{cluster} $S_k$,
        \item the goal is to minimize
        \beq
            \sum_{k=1}^K\sum_{\bm{x}_n \in S_k} \|\bm{x}_n - \bm{\mu}_k\|^2,
        \eeq
        \item since no $y_n$ is involved in the above minimization, this is an example of unsupervised learning,
        \bi
            \item unsupervised learning does not 'corrupt' the data,
        \ei
        \item in general this is an NP hard problem.
    \ei
    \item Lloyd's algorithm:
    \bi
        \item \emph{Lloyd's algorithm} is an iterative solution to the $K$-means clustering:
        \bi
            \item given $S_k$, update $\bm{\mu}_k$,
            \beq\label{k_means_update}
                \bm{u}_k = \frac{1}{|S_k|}\sum_{\bm{x}_n \in S_k} \bm{x}_n,
            \eeq
            \item given $\bm{\mu}_k$, update $S_k$ so that each sample picks the nearest cluster.
        \ei
    \ei
\end{itemize}



%\subsection{Research}
%\begin{itemize}
%    \item ECC and pattern recognition:
%    \bi
%        \item consider a binary pattern classification problem, i.e. the output is one of two possibilities,
%        \item assume input is $(28 \times 28)$ black and white pixels,
%        \item the problem is to map $2^{784}$ patterns to $2$, such that error rate is minimized,
%        \item in ECC:
%        \bi
%            \item this corresponds to $k=1$, and a redundancy of $r=783$,
%            \item we can choose the two signals, i.e. repetition code,
%            \item simple noise models are used, such AWGN,
%            \item simple distance metric could be defined, such as Hamming or Euclidean,
%        \ei
%        \item in ANN:
%        \bi
%            \item we can not choose the signal,
%            \item signal is not unique (deterministic noise)
%            \item noise is difficult to define,
%            \item $\Rightarrow$ distance is hard to define,
%        \ei
%        \item questions:
%        \bi
%            \item how to characterize a binary classification problem?
%            \bi
%                \item options include Kolmogorov complexity, compression ratio etc,
%            \ei
%            \item given a problem, what can we say about
%            \bi
%                \item depth of layers?
%                \item size of each layer?
%                \item activation?
%            \ei
%            \item there are parts to this, whether ANN can handle such a complexity and whether it can learn it.
%        \ei
%    \ei
%\end{itemize}

\newpage
\small
\bibliographystyle{ieeetr}
\bibliography{dl}
\normalsize
\printindex
%\end{appendices}
\end{document}
